{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Final Project: Transformer-Based Question Answering System\n",
    "## Building an Extractive QA and Response Generation Pipeline with SQuAD 2.0\n",
    "\n",
    "    **Student:** Sruthi Puthiyandy\n",
    "    **Student ID:** 162489\n",
    "    **Date:** December 2025  \n",
    "    **Course:** Natural Language Processing\n",
    "\n",
    "---\n",
    "\n",
    "## Project Overview\n",
    "This project implements a complete Question Answering system using state-of-the-art Transformer models:\n",
    "1. **Extractive QA**: Fine-tuned BERT to extract answer spans from passages\n",
    "2. **Response Generation**: Fine-tuned GPT-2 to generate explanatory responses\n",
    "3. **Interactive Demo**: Gradio interface for real-time QA\n",
    "4. **Dataset**: SQuAD 2.0 (Stanford Question Answering Dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n",
      "Collecting datasets\n",
      "  Downloading datasets-4.4.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting torch\n",
      "  Using cached torch-2.9.1-cp313-cp313-win_amd64.whl.metadata (30 kB)\n",
      "Collecting torchvision\n",
      "  Using cached torchvision-0.24.1-cp313-cp313-win_amd64.whl.metadata (5.9 kB)\n",
      "Collecting torchaudio\n",
      "  Using cached torchaudio-2.9.1-cp313-cp313-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting filelock (from transformers)\n",
      "  Using cached filelock-3.20.0-py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting huggingface-hub<1.0,>=0.34.0 (from transformers)\n",
      "  Downloading huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (2.2.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (2025.11.3)\n",
      "Requirement already satisfied: requests in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (2.32.4)\n",
      "Collecting tokenizers<=0.23.0,>=0.22.0 (from transformers)\n",
      "  Downloading tokenizers-0.22.1-cp39-abi3-win_amd64.whl.metadata (6.9 kB)\n",
      "Collecting safetensors>=0.4.3 (from transformers)\n",
      "  Downloading safetensors-0.7.0-cp38-abi3-win_amd64.whl.metadata (4.2 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from transformers) (4.67.1)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
      "  Downloading fsspec-2025.12.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
      "Collecting pyarrow>=21.0.0 (from datasets)\n",
      "  Downloading pyarrow-22.0.0-cp313-cp313-win_amd64.whl.metadata (3.3 kB)\n",
      "Collecting dill<0.4.1,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.4.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from datasets) (2.3.1)\n",
      "Requirement already satisfied: httpx<1.0.0 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from datasets) (0.28.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.6.0-cp313-cp313-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.19 (from datasets)\n",
      "  Downloading multiprocess-0.70.18-py313-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec>=2023.5.0 (from huggingface-hub<1.0,>=0.34.0->transformers)\n",
      "  Using cached fsspec-2025.10.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohttp-3.13.2-cp313-cp313-win_amd64.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: anyio in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from httpx<1.0.0->datasets) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from httpx<1.0.0->datasets) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx>=2.5.1 (from torch)\n",
      "  Downloading networkx-3.6.1-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from torch) (80.9.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from torchvision) (11.2.1)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.4.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading aiosignal-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading frozenlist-1.8.0-cp313-cp313-win_amd64.whl.metadata (21 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading multidict-6.7.0-cp313-cp313-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading propcache-0.4.1-cp313-cp313-win_amd64.whl.metadata (14 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.10.0,>=2023.1.0->datasets)\n",
      "  Downloading yarl-1.22.0-cp313-cp313-win_amd64.whl.metadata (77 kB)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from requests->transformers) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from requests->transformers) (2.4.0)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from jinja2->torch) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading transformers-4.57.3-py3-none-any.whl (12.0 MB)\n",
      "   ---------------------------------------- 0.0/12.0 MB ? eta -:--:--\n",
      "   ----------- ---------------------------- 3.4/12.0 MB 21.0 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 8.4/12.0 MB 21.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.0/12.0 MB 21.7 MB/s  0:00:00\n",
      "Downloading huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\n",
      "   ---------------------------------------- 0.0/566.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 566.1/566.1 kB 31.6 MB/s  0:00:00\n",
      "Downloading tokenizers-0.22.1-cp39-abi3-win_amd64.whl (2.7 MB)\n",
      "   ---------------------------------------- 0.0/2.7 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.7/2.7 MB 28.8 MB/s  0:00:00\n",
      "Downloading datasets-4.4.1-py3-none-any.whl (511 kB)\n",
      "Downloading dill-0.4.0-py3-none-any.whl (119 kB)\n",
      "Using cached fsspec-2025.10.0-py3-none-any.whl (200 kB)\n",
      "Downloading multiprocess-0.70.18-py313-none-any.whl (151 kB)\n",
      "Using cached torch-2.9.1-cp313-cp313-win_amd64.whl (110.9 MB)\n",
      "Using cached torchvision-0.24.1-cp313-cp313-win_amd64.whl (4.3 MB)\n",
      "Using cached torchaudio-2.9.1-cp313-cp313-win_amd64.whl (665 kB)\n",
      "Downloading aiohttp-3.13.2-cp313-cp313-win_amd64.whl (452 kB)\n",
      "Downloading multidict-6.7.0-cp313-cp313-win_amd64.whl (45 kB)\n",
      "Downloading yarl-1.22.0-cp313-cp313-win_amd64.whl (86 kB)\n",
      "Downloading aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Downloading aiosignal-1.4.0-py3-none-any.whl (7.5 kB)\n",
      "Downloading frozenlist-1.8.0-cp313-cp313-win_amd64.whl (43 kB)\n",
      "Downloading networkx-3.6.1-py3-none-any.whl (2.1 MB)\n",
      "   ---------------------------------------- 0.0/2.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.1/2.1 MB 31.1 MB/s  0:00:00\n",
      "Downloading propcache-0.4.1-cp313-cp313-win_amd64.whl (40 kB)\n",
      "Downloading pyarrow-22.0.0-cp313-cp313-win_amd64.whl (28.0 MB)\n",
      "   ---------------------------------------- 0.0/28.0 MB ? eta -:--:--\n",
      "   ---------------- ----------------------- 11.8/28.0 MB 54.2 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 24.1/28.0 MB 56.0 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 28.0/28.0 MB 51.9 MB/s  0:00:00\n",
      "Downloading safetensors-0.7.0-cp38-abi3-win_amd64.whl (341 kB)\n",
      "Using cached sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Using cached filelock-3.20.0-py3-none-any.whl (16 kB)\n",
      "Downloading xxhash-3.6.0-cp313-cp313-win_amd64.whl (31 kB)\n",
      "Installing collected packages: mpmath, xxhash, sympy, safetensors, pyarrow, propcache, networkx, multidict, fsspec, frozenlist, filelock, dill, aiohappyeyeballs, yarl, torch, multiprocess, huggingface-hub, aiosignal, torchvision, torchaudio, tokenizers, aiohttp, transformers, datasets\n",
      "\n",
      "   ----------------------------------------  0/24 [mpmath]\n",
      "   ----------------------------------------  0/24 [mpmath]\n",
      "   ----------------------------------------  0/24 [mpmath]\n",
      "   ----------------------------------------  0/24 [mpmath]\n",
      "   ----------------------------------------  0/24 [mpmath]\n",
      "   ----------------------------------------  0/24 [mpmath]\n",
      "   ----------------------------------------  0/24 [mpmath]\n",
      "   ----------------------------------------  0/24 [mpmath]\n",
      "   ----------------------------------------  0/24 [mpmath]\n",
      "   ----------------------------------------  0/24 [mpmath]\n",
      "   ----------------------------------------  0/24 [mpmath]\n",
      "   ----------------------------------------  0/24 [mpmath]\n",
      "   ----------------------------------------  0/24 [mpmath]\n",
      "   ----------------------------------------  0/24 [mpmath]\n",
      "   ----------------------------------------  0/24 [mpmath]\n",
      "   ----------------------------------------  0/24 [mpmath]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   --- ------------------------------------  2/24 [sympy]\n",
      "   ----- ----------------------------------  3/24 [safetensors]\n",
      "  Attempting uninstall: pyarrow\n",
      "   ----- ----------------------------------  3/24 [safetensors]\n",
      "    Found existing installation: pyarrow 20.0.0\n",
      "   ----- ----------------------------------  3/24 [safetensors]\n",
      "   ------ ---------------------------------  4/24 [pyarrow]\n",
      "    Uninstalling pyarrow-20.0.0:\n",
      "   ------ ---------------------------------  4/24 [pyarrow]\n",
      "   ------ ---------------------------------  4/24 [pyarrow]\n",
      "      Successfully uninstalled pyarrow-20.0.0\n",
      "   ------ ---------------------------------  4/24 [pyarrow]\n",
      "   ------ ---------------------------------  4/24 [pyarrow]\n",
      "   ------ ---------------------------------  4/24 [pyarrow]\n",
      "   ------ ---------------------------------  4/24 [pyarrow]\n",
      "   ------ ---------------------------------  4/24 [pyarrow]\n",
      "   ------ ---------------------------------  4/24 [pyarrow]\n",
      "   ------ ---------------------------------  4/24 [pyarrow]\n",
      "   ------ ---------------------------------  4/24 [pyarrow]\n",
      "   ------ ---------------------------------  4/24 [pyarrow]\n",
      "   ------ ---------------------------------  4/24 [pyarrow]\n",
      "   ------ ---------------------------------  4/24 [pyarrow]\n",
      "   ------ ---------------------------------  4/24 [pyarrow]\n",
      "   ------ ---------------------------------  4/24 [pyarrow]\n",
      "   ------ ---------------------------------  4/24 [pyarrow]\n",
      "   ------ ---------------------------------  4/24 [pyarrow]\n",
      "   ------ ---------------------------------  4/24 [pyarrow]\n",
      "   ------ ---------------------------------  4/24 [pyarrow]\n",
      "   ------ ---------------------------------  4/24 [pyarrow]\n",
      "   ------ ---------------------------------  4/24 [pyarrow]\n",
      "   ------ ---------------------------------  4/24 [pyarrow]\n",
      "   ------ ---------------------------------  4/24 [pyarrow]\n",
      "   ------ ---------------------------------  4/24 [pyarrow]\n",
      "   ------ ---------------------------------  4/24 [pyarrow]\n",
      "   ------ ---------------------------------  4/24 [pyarrow]\n",
      "   ------ ---------------------------------  4/24 [pyarrow]\n",
      "   ------ ---------------------------------  4/24 [pyarrow]\n",
      "   ------ ---------------------------------  4/24 [pyarrow]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ---------- -----------------------------  6/24 [networkx]\n",
      "   ----------- ----------------------------  7/24 [multidict]\n",
      "   ------------- --------------------------  8/24 [fsspec]\n",
      "   ------------- --------------------------  8/24 [fsspec]\n",
      "   ------------- --------------------------  8/24 [fsspec]\n",
      "   ------------- --------------------------  8/24 [fsspec]\n",
      "   ------------- --------------------------  8/24 [fsspec]\n",
      "   ------------- --------------------------  8/24 [fsspec]\n",
      "   ------------- --------------------------  8/24 [fsspec]\n",
      "   ------------- --------------------------  8/24 [fsspec]\n",
      "   ------------- --------------------------  8/24 [fsspec]\n",
      "   ------------- --------------------------  8/24 [fsspec]\n",
      "   --------------- ------------------------  9/24 [frozenlist]\n",
      "   ---------------- ----------------------- 10/24 [filelock]\n",
      "   ------------------ --------------------- 11/24 [dill]\n",
      "   ------------------ --------------------- 11/24 [dill]\n",
      "   ------------------ --------------------- 11/24 [dill]\n",
      "   ------------------ --------------------- 11/24 [dill]\n",
      "   ------------------ --------------------- 11/24 [dill]\n",
      "   ------------------ --------------------- 11/24 [dill]\n",
      "   ------------------ --------------------- 11/24 [dill]\n",
      "   ------------------ --------------------- 11/24 [dill]\n",
      "   -------------------- ------------------- 12/24 [aiohappyeyeballs]\n",
      "   --------------------- ------------------ 13/24 [yarl]\n",
      "   --------------------- ------------------ 13/24 [yarl]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ----------------------- ---------------- 14/24 [torch]\n",
      "   ------------------------- -------------- 15/24 [multiprocess]\n",
      "   ------------------------- -------------- 15/24 [multiprocess]\n",
      "   ------------------------- -------------- 15/24 [multiprocess]\n",
      "   ------------------------- -------------- 15/24 [multiprocess]\n",
      "   ------------------------- -------------- 15/24 [multiprocess]\n",
      "   ------------------------- -------------- 15/24 [multiprocess]\n",
      "   ------------------------- -------------- 15/24 [multiprocess]\n",
      "   -------------------------- ------------- 16/24 [huggingface-hub]\n",
      "   -------------------------- ------------- 16/24 [huggingface-hub]\n",
      "   -------------------------- ------------- 16/24 [huggingface-hub]\n",
      "   -------------------------- ------------- 16/24 [huggingface-hub]\n",
      "   -------------------------- ------------- 16/24 [huggingface-hub]\n",
      "   -------------------------- ------------- 16/24 [huggingface-hub]\n",
      "   -------------------------- ------------- 16/24 [huggingface-hub]\n",
      "   -------------------------- ------------- 16/24 [huggingface-hub]\n",
      "   -------------------------- ------------- 16/24 [huggingface-hub]\n",
      "   -------------------------- ------------- 16/24 [huggingface-hub]\n",
      "   -------------------------- ------------- 16/24 [huggingface-hub]\n",
      "   -------------------------- ------------- 16/24 [huggingface-hub]\n",
      "   -------------------------- ------------- 16/24 [huggingface-hub]\n",
      "   -------------------------- ------------- 16/24 [huggingface-hub]\n",
      "   -------------------------- ------------- 16/24 [huggingface-hub]\n",
      "   -------------------------- ------------- 16/24 [huggingface-hub]\n",
      "   -------------------------- ------------- 16/24 [huggingface-hub]\n",
      "   -------------------------- ------------- 16/24 [huggingface-hub]\n",
      "   -------------------------- ------------- 16/24 [huggingface-hub]\n",
      "   -------------------------- ------------- 16/24 [huggingface-hub]\n",
      "   -------------------------- ------------- 16/24 [huggingface-hub]\n",
      "   -------------------------- ------------- 16/24 [huggingface-hub]\n",
      "   -------------------------- ------------- 16/24 [huggingface-hub]\n",
      "   -------------------------- ------------- 16/24 [huggingface-hub]\n",
      "   -------------------------- ------------- 16/24 [huggingface-hub]\n",
      "   -------------------------- ------------- 16/24 [huggingface-hub]\n",
      "   -------------------------- ------------- 16/24 [huggingface-hub]\n",
      "   -------------------------- ------------- 16/24 [huggingface-hub]\n",
      "   -------------------------- ------------- 16/24 [huggingface-hub]\n",
      "   -------------------------- ------------- 16/24 [huggingface-hub]\n",
      "   -------------------------- ------------- 16/24 [huggingface-hub]\n",
      "   ------------------------------ --------- 18/24 [torchvision]\n",
      "   ------------------------------ --------- 18/24 [torchvision]\n",
      "   ------------------------------ --------- 18/24 [torchvision]\n",
      "   ------------------------------ --------- 18/24 [torchvision]\n",
      "   ------------------------------ --------- 18/24 [torchvision]\n",
      "   ------------------------------ --------- 18/24 [torchvision]\n",
      "   ------------------------------ --------- 18/24 [torchvision]\n",
      "   ------------------------------ --------- 18/24 [torchvision]\n",
      "   ------------------------------ --------- 18/24 [torchvision]\n",
      "   ------------------------------ --------- 18/24 [torchvision]\n",
      "   ------------------------------ --------- 18/24 [torchvision]\n",
      "   ------------------------------ --------- 18/24 [torchvision]\n",
      "   ------------------------------ --------- 18/24 [torchvision]\n",
      "   ------------------------------ --------- 18/24 [torchvision]\n",
      "   ------------------------------ --------- 18/24 [torchvision]\n",
      "   ------------------------------ --------- 18/24 [torchvision]\n",
      "   ------------------------------ --------- 18/24 [torchvision]\n",
      "   ------------------------------ --------- 18/24 [torchvision]\n",
      "   ------------------------------ --------- 18/24 [torchvision]\n",
      "   ------------------------------ --------- 18/24 [torchvision]\n",
      "   ------------------------------ --------- 18/24 [torchvision]\n",
      "   ------------------------------ --------- 18/24 [torchvision]\n",
      "   ------------------------------ --------- 18/24 [torchvision]\n",
      "   ------------------------------ --------- 18/24 [torchvision]\n",
      "   ------------------------------ --------- 18/24 [torchvision]\n",
      "   ------------------------------ --------- 18/24 [torchvision]\n",
      "   ------------------------------ --------- 18/24 [torchvision]\n",
      "   ------------------------------ --------- 18/24 [torchvision]\n",
      "   ------------------------------ --------- 18/24 [torchvision]\n",
      "   ------------------------------ --------- 18/24 [torchvision]\n",
      "   ------------------------------- -------- 19/24 [torchaudio]\n",
      "   ------------------------------- -------- 19/24 [torchaudio]\n",
      "   ------------------------------- -------- 19/24 [torchaudio]\n",
      "   ------------------------------- -------- 19/24 [torchaudio]\n",
      "   ------------------------------- -------- 19/24 [torchaudio]\n",
      "   ------------------------------- -------- 19/24 [torchaudio]\n",
      "   ------------------------------- -------- 19/24 [torchaudio]\n",
      "   ------------------------------- -------- 19/24 [torchaudio]\n",
      "   ------------------------------- -------- 19/24 [torchaudio]\n",
      "   ------------------------------- -------- 19/24 [torchaudio]\n",
      "   ------------------------------- -------- 19/24 [torchaudio]\n",
      "   ------------------------------- -------- 19/24 [torchaudio]\n",
      "   ------------------------------- -------- 19/24 [torchaudio]\n",
      "   --------------------------------- ------ 20/24 [tokenizers]\n",
      "   --------------------------------- ------ 20/24 [tokenizers]\n",
      "   ----------------------------------- ---- 21/24 [aiohttp]\n",
      "   ----------------------------------- ---- 21/24 [aiohttp]\n",
      "   ----------------------------------- ---- 21/24 [aiohttp]\n",
      "   ----------------------------------- ---- 21/24 [aiohttp]\n",
      "   ----------------------------------- ---- 21/24 [aiohttp]\n",
      "   ----------------------------------- ---- 21/24 [aiohttp]\n",
      "   ----------------------------------- ---- 21/24 [aiohttp]\n",
      "   ----------------------------------- ---- 21/24 [aiohttp]\n",
      "   ----------------------------------- ---- 21/24 [aiohttp]\n",
      "   ----------------------------------- ---- 21/24 [aiohttp]\n",
      "   ----------------------------------- ---- 21/24 [aiohttp]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   ------------------------------------ --- 22/24 [transformers]\n",
      "   -------------------------------------- - 23/24 [datasets]\n",
      "   -------------------------------------- - 23/24 [datasets]\n",
      "   -------------------------------------- - 23/24 [datasets]\n",
      "   -------------------------------------- - 23/24 [datasets]\n",
      "   -------------------------------------- - 23/24 [datasets]\n",
      "   -------------------------------------- - 23/24 [datasets]\n",
      "   -------------------------------------- - 23/24 [datasets]\n",
      "   -------------------------------------- - 23/24 [datasets]\n",
      "   -------------------------------------- - 23/24 [datasets]\n",
      "   -------------------------------------- - 23/24 [datasets]\n",
      "   -------------------------------------- - 23/24 [datasets]\n",
      "   -------------------------------------- - 23/24 [datasets]\n",
      "   -------------------------------------- - 23/24 [datasets]\n",
      "   -------------------------------------- - 23/24 [datasets]\n",
      "   -------------------------------------- - 23/24 [datasets]\n",
      "   -------------------------------------- - 23/24 [datasets]\n",
      "   -------------------------------------- - 23/24 [datasets]\n",
      "   -------------------------------------- - 23/24 [datasets]\n",
      "   -------------------------------------- - 23/24 [datasets]\n",
      "   ---------------------------------------- 24/24 [datasets]\n",
      "\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.13.2 aiosignal-1.4.0 datasets-4.4.1 dill-0.4.0 filelock-3.20.0 frozenlist-1.8.0 fsspec-2025.10.0 huggingface-hub-0.36.0 mpmath-1.3.0 multidict-6.7.0 multiprocess-0.70.18 networkx-3.6.1 propcache-0.4.1 pyarrow-22.0.0 safetensors-0.7.0 sympy-1.14.0 tokenizers-0.22.1 torch-2.9.1 torchaudio-2.9.1 torchvision-0.24.1 transformers-4.57.3 xxhash-3.6.0 yarl-1.22.0\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting gradio\n",
      "  Downloading gradio-6.1.0-py3-none-any.whl.metadata (16 kB)\n",
      "Collecting bertviz\n",
      "  Downloading bertviz-1.4.1-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting wordcloud\n",
      "  Downloading wordcloud-1.9.4-cp313-cp313-win_amd64.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (3.10.1)\n",
      "Requirement already satisfied: seaborn in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (0.13.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (2.3.1)\n",
      "Requirement already satisfied: numpy in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (2.2.5)\n",
      "Collecting aiofiles<25.0,>=22.0 (from gradio)\n",
      "  Downloading aiofiles-24.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (4.11.0)\n",
      "Collecting audioop-lts<1.0 (from gradio)\n",
      "  Downloading audioop_lts-0.2.2-cp313-abi3-win_amd64.whl.metadata (2.0 kB)\n",
      "Collecting brotli>=1.1.0 (from gradio)\n",
      "  Downloading brotli-1.2.0-cp313-cp313-win_amd64.whl.metadata (6.3 kB)\n",
      "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
      "  Downloading fastapi-0.124.2-py3-none-any.whl.metadata (30 kB)\n",
      "Collecting ffmpy (from gradio)\n",
      "  Downloading ffmpy-1.0.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting gradio-client==2.0.1 (from gradio)\n",
      "  Downloading gradio_client-2.0.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Collecting groovy~=0.1 (from gradio)\n",
      "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: httpx<1.0,>=0.24.1 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (0.28.1)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.33.5 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (0.36.0)\n",
      "Requirement already satisfied: jinja2<4.0 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (3.1.6)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (3.0.2)\n",
      "Collecting orjson~=3.0 (from gradio)\n",
      "  Downloading orjson-3.11.5-cp313-cp313-win_amd64.whl.metadata (42 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (25.0)\n",
      "Requirement already satisfied: pillow<13.0,>=8.0 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (11.2.1)\n",
      "Collecting pydantic<=2.12.4,>=2.11.10 (from gradio)\n",
      "  Downloading pydantic-2.12.4-py3-none-any.whl.metadata (89 kB)\n",
      "Collecting pydub (from gradio)\n",
      "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting python-multipart>=0.0.18 (from gradio)\n",
      "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (6.0.3)\n",
      "Collecting safehttpx<0.2.0,>=0.1.7 (from gradio)\n",
      "  Downloading safehttpx-0.1.7-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting semantic-version~=2.0 (from gradio)\n",
      "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
      "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
      "  Downloading starlette-0.50.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
      "  Downloading tomlkit-0.13.3-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting typer<1.0,>=0.12 (from gradio)\n",
      "  Downloading typer-0.20.0-py3-none-any.whl.metadata (16 kB)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from gradio) (4.14.1)\n",
      "Collecting uvicorn>=0.14.0 (from gradio)\n",
      "  Downloading uvicorn-0.38.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: fsspec in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from gradio-client==2.0.1->gradio) (2025.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
      "Collecting annotated-doc>=0.0.2 (from fastapi<1.0,>=0.115.2->gradio)\n",
      "  Downloading annotated_doc-0.0.4-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: certifi in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from httpx<1.0,>=0.24.1->gradio) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (3.20.0)\n",
      "Requirement already satisfied: requests in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (4.67.1)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic<=2.12.4,>=2.11.10->gradio)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.41.5 (from pydantic<=2.12.4,>=2.11.10->gradio)\n",
      "  Downloading pydantic_core-2.41.5-cp313-cp313-win_amd64.whl.metadata (7.4 kB)\n",
      "Collecting typing-inspection>=0.4.2 (from pydantic<=2.12.4,>=2.11.10->gradio)\n",
      "  Downloading typing_inspection-0.4.2-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0,>=0.12->gradio)\n",
      "  Downloading shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from typer<1.0,>=0.12->gradio) (14.2.0)\n",
      "Requirement already satisfied: transformers>=2.0 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from bertviz) (4.57.3)\n",
      "Requirement already satisfied: torch>=1.0 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from bertviz) (2.9.1)\n",
      "Collecting boto3 (from bertviz)\n",
      "  Downloading boto3-1.42.7-py3-none-any.whl.metadata (6.8 kB)\n",
      "Requirement already satisfied: regex in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from bertviz) (2025.11.3)\n",
      "Collecting sentencepiece (from bertviz)\n",
      "  Downloading sentencepiece-0.2.1-cp313-cp313-win_amd64.whl.metadata (10 kB)\n",
      "Requirement already satisfied: IPython>=7.14 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from bertviz) (9.5.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from click>=8.0.0->typer<1.0,>=0.12->gradio) (0.4.6)\n",
      "Requirement already satisfied: decorator in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from IPython>=7.14->bertviz) (5.2.1)\n",
      "Requirement already satisfied: ipython-pygments-lexers in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from IPython>=7.14->bertviz) (1.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from IPython>=7.14->bertviz) (0.19.2)\n",
      "Requirement already satisfied: matplotlib-inline in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from IPython>=7.14->bertviz) (0.1.7)\n",
      "Requirement already satisfied: prompt_toolkit<3.1.0,>=3.0.41 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from IPython>=7.14->bertviz) (3.0.52)\n",
      "Requirement already satisfied: pygments>=2.4.0 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from IPython>=7.14->bertviz) (2.19.2)\n",
      "Requirement already satisfied: stack_data in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from IPython>=7.14->bertviz) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from IPython>=7.14->bertviz) (5.14.3)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from prompt_toolkit<3.1.0,>=3.0.41->IPython>=7.14->bertviz) (0.2.14)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from jedi>=0.16->IPython>=7.14->bertviz) (0.8.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from torch>=1.0->bertviz) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from torch>=1.0->bertviz) (3.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from torch>=1.0->bertviz) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from sympy>=1.13.3->torch>=1.0->bertviz) (1.3.0)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from transformers>=2.0->bertviz) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from transformers>=2.0->bertviz) (0.7.0)\n",
      "Collecting botocore<1.43.0,>=1.42.7 (from boto3->bertviz)\n",
      "  Downloading botocore-1.42.7-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3->bertviz)\n",
      "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting s3transfer<0.17.0,>=0.16.0 (from boto3->bertviz)\n",
      "  Downloading s3transfer-0.16.0-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from botocore<1.43.0,>=1.42.7->boto3->bertviz) (2.4.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (3.4.2)\n",
      "Requirement already satisfied: executing>=1.2.0 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from stack_data->IPython>=7.14->bertviz) (2.2.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from stack_data->IPython>=7.14->bertviz) (3.0.0)\n",
      "Requirement already satisfied: pure-eval in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from stack_data->IPython>=7.14->bertviz) (0.2.3)\n",
      "Downloading gradio-6.1.0-py3-none-any.whl (23.0 MB)\n",
      "   ---------------------------------------- 0.0/23.0 MB ? eta -:--:--\n",
      "   --- ------------------------------------ 2.1/23.0 MB 10.6 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 5.5/23.0 MB 14.0 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 10.2/23.0 MB 16.6 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 16.5/23.0 MB 20.2 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 21.0/23.0 MB 20.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 23.0/23.0 MB 19.5 MB/s  0:00:01\n",
      "Downloading gradio_client-2.0.1-py3-none-any.whl (55 kB)\n",
      "Downloading aiofiles-24.1.0-py3-none-any.whl (15 kB)\n",
      "Downloading audioop_lts-0.2.2-cp313-abi3-win_amd64.whl (30 kB)\n",
      "Downloading fastapi-0.124.2-py3-none-any.whl (112 kB)\n",
      "Downloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
      "Downloading orjson-3.11.5-cp313-cp313-win_amd64.whl (133 kB)\n",
      "Downloading pydantic-2.12.4-py3-none-any.whl (463 kB)\n",
      "Downloading pydantic_core-2.41.5-cp313-cp313-win_amd64.whl (2.0 MB)\n",
      "   ---------------------------------------- 0.0/2.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 2.0/2.0 MB 34.6 MB/s  0:00:00\n",
      "Downloading safehttpx-0.1.7-py3-none-any.whl (9.0 kB)\n",
      "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
      "Downloading starlette-0.50.0-py3-none-any.whl (74 kB)\n",
      "Downloading tomlkit-0.13.3-py3-none-any.whl (38 kB)\n",
      "Downloading typer-0.20.0-py3-none-any.whl (47 kB)\n",
      "Downloading bertviz-1.4.1-py3-none-any.whl (157 kB)\n",
      "Downloading wordcloud-1.9.4-cp313-cp313-win_amd64.whl (300 kB)\n",
      "Downloading annotated_doc-0.0.4-py3-none-any.whl (5.3 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Downloading brotli-1.2.0-cp313-cp313-win_amd64.whl (369 kB)\n",
      "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
      "Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Downloading typing_inspection-0.4.2-py3-none-any.whl (14 kB)\n",
      "Downloading uvicorn-0.38.0-py3-none-any.whl (68 kB)\n",
      "Downloading boto3-1.42.7-py3-none-any.whl (140 kB)\n",
      "Downloading botocore-1.42.7-py3-none-any.whl (14.5 MB)\n",
      "   ---------------------------------------- 0.0/14.5 MB ? eta -:--:--\n",
      "   --------------- ------------------------ 5.8/14.5 MB 35.0 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 13.4/14.5 MB 34.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 14.5/14.5 MB 32.2 MB/s  0:00:00\n",
      "Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Downloading s3transfer-0.16.0-py3-none-any.whl (86 kB)\n",
      "Downloading ffmpy-1.0.0-py3-none-any.whl (5.6 kB)\n",
      "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Downloading sentencepiece-0.2.1-cp313-cp313-win_amd64.whl (1.1 MB)\n",
      "   ---------------------------------------- 0.0/1.1 MB ? eta -:--:--\n",
      "   ---------------------------------------- 1.1/1.1 MB 57.0 MB/s  0:00:00\n",
      "Installing collected packages: pydub, brotli, typing-inspection, tomlkit, shellingham, sentencepiece, semantic-version, python-multipart, pydantic-core, orjson, jmespath, groovy, ffmpy, audioop-lts, annotated-types, annotated-doc, aiofiles, uvicorn, starlette, pydantic, botocore, wordcloud, typer, safehttpx, s3transfer, gradio-client, fastapi, gradio, boto3, bertviz\n",
      "\n",
      "   ----------------------------------------  0/30 [pydub]\n",
      "   - --------------------------------------  1/30 [brotli]\n",
      "   ---- -----------------------------------  3/30 [tomlkit]\n",
      "   ---- -----------------------------------  3/30 [tomlkit]\n",
      "   ----- ----------------------------------  4/30 [shellingham]\n",
      "   ------ ---------------------------------  5/30 [sentencepiece]\n",
      "   -------- -------------------------------  6/30 [semantic-version]\n",
      "   --------- ------------------------------  7/30 [python-multipart]\n",
      "   ---------- -----------------------------  8/30 [pydantic-core]\n",
      "   ------------- -------------------------- 10/30 [jmespath]\n",
      "   ------------- -------------------------- 10/30 [jmespath]\n",
      "   ------------------ --------------------- 14/30 [annotated-types]\n",
      "   --------------------- ------------------ 16/30 [aiofiles]\n",
      "   ---------------------- ----------------- 17/30 [uvicorn]\n",
      "   ---------------------- ----------------- 17/30 [uvicorn]\n",
      "   ---------------------- ----------------- 17/30 [uvicorn]\n",
      "   ---------------------- ----------------- 17/30 [uvicorn]\n",
      "   ---------------------- ----------------- 17/30 [uvicorn]\n",
      "   ------------------------ --------------- 18/30 [starlette]\n",
      "   ------------------------ --------------- 18/30 [starlette]\n",
      "   ------------------------ --------------- 18/30 [starlette]\n",
      "   ------------------------ --------------- 18/30 [starlette]\n",
      "   ------------------------ --------------- 18/30 [starlette]\n",
      "   ------------------------ --------------- 18/30 [starlette]\n",
      "   ------------------------- -------------- 19/30 [pydantic]\n",
      "   ------------------------- -------------- 19/30 [pydantic]\n",
      "   ------------------------- -------------- 19/30 [pydantic]\n",
      "   ------------------------- -------------- 19/30 [pydantic]\n",
      "   ------------------------- -------------- 19/30 [pydantic]\n",
      "   ------------------------- -------------- 19/30 [pydantic]\n",
      "   ------------------------- -------------- 19/30 [pydantic]\n",
      "   ------------------------- -------------- 19/30 [pydantic]\n",
      "   ------------------------- -------------- 19/30 [pydantic]\n",
      "   ------------------------- -------------- 19/30 [pydantic]\n",
      "   ------------------------- -------------- 19/30 [pydantic]\n",
      "   ------------------------- -------------- 19/30 [pydantic]\n",
      "   ------------------------- -------------- 19/30 [pydantic]\n",
      "   ------------------------- -------------- 19/30 [pydantic]\n",
      "   ------------------------- -------------- 19/30 [pydantic]\n",
      "   ------------------------- -------------- 19/30 [pydantic]\n",
      "   ------------------------- -------------- 19/30 [pydantic]\n",
      "   ------------------------- -------------- 19/30 [pydantic]\n",
      "   -------------------------- ------------- 20/30 [botocore]\n",
      "   -------------------------- ------------- 20/30 [botocore]\n",
      "   -------------------------- ------------- 20/30 [botocore]\n",
      "   -------------------------- ------------- 20/30 [botocore]\n",
      "   -------------------------- ------------- 20/30 [botocore]\n",
      "   -------------------------- ------------- 20/30 [botocore]\n",
      "   -------------------------- ------------- 20/30 [botocore]\n",
      "   -------------------------- ------------- 20/30 [botocore]\n",
      "   -------------------------- ------------- 20/30 [botocore]\n",
      "   -------------------------- ------------- 20/30 [botocore]\n",
      "   -------------------------- ------------- 20/30 [botocore]\n",
      "   -------------------------- ------------- 20/30 [botocore]\n",
      "   -------------------------- ------------- 20/30 [botocore]\n",
      "   -------------------------- ------------- 20/30 [botocore]\n",
      "   -------------------------- ------------- 20/30 [botocore]\n",
      "   -------------------------- ------------- 20/30 [botocore]\n",
      "   -------------------------- ------------- 20/30 [botocore]\n",
      "   -------------------------- ------------- 20/30 [botocore]\n",
      "   -------------------------- ------------- 20/30 [botocore]\n",
      "   -------------------------- ------------- 20/30 [botocore]\n",
      "   ---------------------------- ----------- 21/30 [wordcloud]\n",
      "   ---------------------------- ----------- 21/30 [wordcloud]\n",
      "   ----------------------------- ---------- 22/30 [typer]\n",
      "   ----------------------------- ---------- 22/30 [typer]\n",
      "   ----------------------------- ---------- 22/30 [typer]\n",
      "   ----------------------------- ---------- 22/30 [typer]\n",
      "   -------------------------------- ------- 24/30 [s3transfer]\n",
      "   -------------------------------- ------- 24/30 [s3transfer]\n",
      "   -------------------------------- ------- 24/30 [s3transfer]\n",
      "   --------------------------------- ------ 25/30 [gradio-client]\n",
      "   --------------------------------- ------ 25/30 [gradio-client]\n",
      "   ---------------------------------- ----- 26/30 [fastapi]\n",
      "   ---------------------------------- ----- 26/30 [fastapi]\n",
      "   ---------------------------------- ----- 26/30 [fastapi]\n",
      "   ---------------------------------- ----- 26/30 [fastapi]\n",
      "   ---------------------------------- ----- 26/30 [fastapi]\n",
      "   ---------------------------------- ----- 26/30 [fastapi]\n",
      "   ---------------------------------- ----- 26/30 [fastapi]\n",
      "   ---------------------------------- ----- 26/30 [fastapi]\n",
      "   ---------------------------------- ----- 26/30 [fastapi]\n",
      "   ---------------------------------- ----- 26/30 [fastapi]\n",
      "   ------------------------------------ --- 27/30 [gradio]\n",
      "   ------------------------------------ --- 27/30 [gradio]\n",
      "   ------------------------------------ --- 27/30 [gradio]\n",
      "   ------------------------------------ --- 27/30 [gradio]\n",
      "   ------------------------------------ --- 27/30 [gradio]\n",
      "   ------------------------------------ --- 27/30 [gradio]\n",
      "   ------------------------------------ --- 27/30 [gradio]\n",
      "   ------------------------------------ --- 27/30 [gradio]\n",
      "   ------------------------------------ --- 27/30 [gradio]\n",
      "   ------------------------------------ --- 27/30 [gradio]\n",
      "   ------------------------------------ --- 27/30 [gradio]\n",
      "   ------------------------------------ --- 27/30 [gradio]\n",
      "   ------------------------------------ --- 27/30 [gradio]\n",
      "   ------------------------------------ --- 27/30 [gradio]\n",
      "   ------------------------------------ --- 27/30 [gradio]\n",
      "   ------------------------------------ --- 27/30 [gradio]\n",
      "   ------------------------------------ --- 27/30 [gradio]\n",
      "   ------------------------------------ --- 27/30 [gradio]\n",
      "   ------------------------------------ --- 27/30 [gradio]\n",
      "   ------------------------------------ --- 27/30 [gradio]\n",
      "   ------------------------------------ --- 27/30 [gradio]\n",
      "   ------------------------------------ --- 27/30 [gradio]\n",
      "   ------------------------------------ --- 27/30 [gradio]\n",
      "   ------------------------------------ --- 27/30 [gradio]\n",
      "   ------------------------------------ --- 27/30 [gradio]\n",
      "   ------------------------------------ --- 27/30 [gradio]\n",
      "   ------------------------------------ --- 27/30 [gradio]\n",
      "   ------------------------------------ --- 27/30 [gradio]\n",
      "   ------------------------------------ --- 27/30 [gradio]\n",
      "   ------------------------------------ --- 27/30 [gradio]\n",
      "   ------------------------------------ --- 27/30 [gradio]\n",
      "   ------------------------------------ --- 27/30 [gradio]\n",
      "   ------------------------------------ --- 27/30 [gradio]\n",
      "   ------------------------------------ --- 27/30 [gradio]\n",
      "   ------------------------------------ --- 27/30 [gradio]\n",
      "   ------------------------------------ --- 27/30 [gradio]\n",
      "   ------------------------------------ --- 27/30 [gradio]\n",
      "   ------------------------------------ --- 27/30 [gradio]\n",
      "   ------------------------------------ --- 27/30 [gradio]\n",
      "   ------------------------------------ --- 27/30 [gradio]\n",
      "   ------------------------------------ --- 27/30 [gradio]\n",
      "   ------------------------------------ --- 27/30 [gradio]\n",
      "   ------------------------------------- -- 28/30 [boto3]\n",
      "   ------------------------------------- -- 28/30 [boto3]\n",
      "   ------------------------------------- -- 28/30 [boto3]\n",
      "   ------------------------------------- -- 28/30 [boto3]\n",
      "   ------------------------------------- -- 28/30 [boto3]\n",
      "   ------------------------------------- -- 28/30 [boto3]\n",
      "   -------------------------------------- - 29/30 [bertviz]\n",
      "   -------------------------------------- - 29/30 [bertviz]\n",
      "   -------------------------------------- - 29/30 [bertviz]\n",
      "   -------------------------------------- - 29/30 [bertviz]\n",
      "   -------------------------------------- - 29/30 [bertviz]\n",
      "   ---------------------------------------- 30/30 [bertviz]\n",
      "\n",
      "Successfully installed aiofiles-24.1.0 annotated-doc-0.0.4 annotated-types-0.7.0 audioop-lts-0.2.2 bertviz-1.4.1 boto3-1.42.7 botocore-1.42.7 brotli-1.2.0 fastapi-0.124.2 ffmpy-1.0.0 gradio-6.1.0 gradio-client-2.0.1 groovy-0.1.2 jmespath-1.0.1 orjson-3.11.5 pydantic-2.12.4 pydantic-core-2.41.5 pydub-0.25.1 python-multipart-0.0.20 s3transfer-0.16.0 safehttpx-0.1.7 semantic-version-2.10.0 sentencepiece-0.2.1 shellingham-1.5.4 starlette-0.50.0 tomlkit-0.13.3 typer-0.20.0 typing-inspection-0.4.2 uvicorn-0.38.0 wordcloud-1.9.4\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-1.12.0-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.6-py3-none-any.whl.metadata (9.5 kB)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (1.7.2)\n",
      "Collecting peft\n",
      "  Downloading peft-0.18.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from accelerate) (2.2.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from accelerate) (25.0)\n",
      "Requirement already satisfied: psutil in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from accelerate) (7.1.0)\n",
      "Requirement already satisfied: pyyaml in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from accelerate) (6.0.3)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from accelerate) (2.9.1)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from accelerate) (0.36.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from accelerate) (0.7.0)\n",
      "Requirement already satisfied: datasets>=2.0.0 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from evaluate) (4.4.1)\n",
      "Requirement already satisfied: dill in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from evaluate) (0.4.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from evaluate) (2.3.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from evaluate) (2.32.4)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from evaluate) (3.6.0)\n",
      "Requirement already satisfied: multiprocess in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from evaluate) (0.70.18)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.10.0)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from scikit-learn) (1.16.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: transformers in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from peft) (4.57.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from datasets>=2.0.0->evaluate) (3.20.0)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from datasets>=2.0.0->evaluate) (22.0.0)\n",
      "Requirement already satisfied: httpx<1.0.0 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from datasets>=2.0.0->evaluate) (0.28.1)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from fsspec[http]>=2021.05.0->evaluate) (3.13.2)\n",
      "Requirement already satisfied: anyio in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (4.11.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from httpx<1.0.0->datasets>=2.0.0->evaluate) (3.10)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from httpcore==1.*->httpx<1.0.0->datasets>=2.0.0->evaluate) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from huggingface_hub>=0.21.0->accelerate) (4.14.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.22.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from requests>=2.19.0->evaluate) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from requests>=2.19.0->evaluate) (2.4.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from torch>=2.0.0->accelerate) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from torch>=2.0.0->accelerate) (3.6.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from torch>=2.0.0->accelerate) (80.9.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from sympy>=1.13.3->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from tqdm>=4.62.1->evaluate) (0.4.6)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from anyio->httpx<1.0.0->datasets>=2.0.0->evaluate) (1.3.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from transformers->peft) (2025.11.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\sruth\\appdata\\roaming\\python\\python313\\site-packages (from transformers->peft) (0.22.1)\n",
      "Downloading accelerate-1.12.0-py3-none-any.whl (380 kB)\n",
      "Downloading evaluate-0.4.6-py3-none-any.whl (84 kB)\n",
      "Downloading peft-0.18.0-py3-none-any.whl (556 kB)\n",
      "   ---------------------------------------- 0.0/556.4 kB ? eta -:--:--\n",
      "   ---------------------------------------- 556.4/556.4 kB 31.0 MB/s  0:00:00\n",
      "Installing collected packages: accelerate, peft, evaluate\n",
      "\n",
      "   ---------------------------------------- 0/3 [accelerate]\n",
      "   ---------------------------------------- 0/3 [accelerate]\n",
      "   ---------------------------------------- 0/3 [accelerate]\n",
      "   ---------------------------------------- 0/3 [accelerate]\n",
      "   ---------------------------------------- 0/3 [accelerate]\n",
      "   ---------------------------------------- 0/3 [accelerate]\n",
      "   ---------------------------------------- 0/3 [accelerate]\n",
      "   ---------------------------------------- 0/3 [accelerate]\n",
      "   ---------------------------------------- 0/3 [accelerate]\n",
      "   ---------------------------------------- 0/3 [accelerate]\n",
      "   ---------------------------------------- 0/3 [accelerate]\n",
      "   ---------------------------------------- 0/3 [accelerate]\n",
      "   ---------------------------------------- 0/3 [accelerate]\n",
      "   ---------------------------------------- 0/3 [accelerate]\n",
      "   ---------------------------------------- 0/3 [accelerate]\n",
      "   ---------------------------------------- 0/3 [accelerate]\n",
      "   ---------------------------------------- 0/3 [accelerate]\n",
      "   ---------------------------------------- 0/3 [accelerate]\n",
      "   ---------------------------------------- 0/3 [accelerate]\n",
      "   ------------- -------------------------- 1/3 [peft]\n",
      "   ------------- -------------------------- 1/3 [peft]\n",
      "   ------------- -------------------------- 1/3 [peft]\n",
      "   ------------- -------------------------- 1/3 [peft]\n",
      "   ------------- -------------------------- 1/3 [peft]\n",
      "   ------------- -------------------------- 1/3 [peft]\n",
      "   ------------- -------------------------- 1/3 [peft]\n",
      "   ------------- -------------------------- 1/3 [peft]\n",
      "   ------------- -------------------------- 1/3 [peft]\n",
      "   ------------- -------------------------- 1/3 [peft]\n",
      "   ------------- -------------------------- 1/3 [peft]\n",
      "   ------------- -------------------------- 1/3 [peft]\n",
      "   ------------- -------------------------- 1/3 [peft]\n",
      "   ------------- -------------------------- 1/3 [peft]\n",
      "   ------------- -------------------------- 1/3 [peft]\n",
      "   ------------- -------------------------- 1/3 [peft]\n",
      "   ------------- -------------------------- 1/3 [peft]\n",
      "   ------------- -------------------------- 1/3 [peft]\n",
      "   ------------- -------------------------- 1/3 [peft]\n",
      "   ------------- -------------------------- 1/3 [peft]\n",
      "   ------------- -------------------------- 1/3 [peft]\n",
      "   ------------- -------------------------- 1/3 [peft]\n",
      "   ------------- -------------------------- 1/3 [peft]\n",
      "   ------------- -------------------------- 1/3 [peft]\n",
      "   ------------- -------------------------- 1/3 [peft]\n",
      "   ------------- -------------------------- 1/3 [peft]\n",
      "   ------------- -------------------------- 1/3 [peft]\n",
      "   ------------- -------------------------- 1/3 [peft]\n",
      "   ------------- -------------------------- 1/3 [peft]\n",
      "   -------------------------- ------------- 2/3 [evaluate]\n",
      "   -------------------------- ------------- 2/3 [evaluate]\n",
      "   -------------------------- ------------- 2/3 [evaluate]\n",
      "   -------------------------- ------------- 2/3 [evaluate]\n",
      "   -------------------------- ------------- 2/3 [evaluate]\n",
      "   -------------------------- ------------- 2/3 [evaluate]\n",
      "   ---------------------------------------- 3/3 [evaluate]\n",
      "\n",
      "Successfully installed accelerate-1.12.0 evaluate-0.4.6 peft-0.18.0\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install transformers datasets torch torchvision torchaudio\n",
    "!pip install gradio bertviz wordcloud matplotlib seaborn pandas numpy\n",
    "!pip install accelerate evaluate scikit-learn peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Your currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\transformers\\activations_tf.py:22\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtf_keras\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m):\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'tf_keras'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m warnings.filterwarnings(\u001b[33m'\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dataset\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     13\u001b[39m     AutoTokenizer, AutoModelForQuestionAnswering, AutoModelForCausalLM,\n\u001b[32m     14\u001b[39m     TrainingArguments, Trainer, DefaultDataCollator,\n\u001b[32m     15\u001b[39m     pipeline, set_seed\n\u001b[32m     16\u001b[39m )\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransformers\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata_collator\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataCollatorForLanguageModeling\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mevaluate\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1412\u001b[39m, in \u001b[36m_handle_fromlist\u001b[39m\u001b[34m(module, fromlist, import_, recursive)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\transformers\\utils\\import_utils.py:2317\u001b[39m, in \u001b[36m_LazyModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   2315\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._class_to_module:\n\u001b[32m   2316\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2317\u001b[39m         module = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2318\u001b[39m         value = \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[32m   2319\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\transformers\\utils\\import_utils.py:2347\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   2345\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib.import_module(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m + module_name, \u001b[38;5;28mself\u001b[39m.\u001b[34m__name__\u001b[39m)\n\u001b[32m   2346\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m2347\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\transformers\\utils\\import_utils.py:2345\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   2343\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m   2344\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2345\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2346\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   2347\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python313\\Lib\\importlib\\__init__.py:88\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     86\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     87\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\transformers\\trainer.py:42\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TYPE_CHECKING, Any, Callable, Optional, Union\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# Integrations must be imported before ML frameworks:\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# ruff: isort: off\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m42\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mintegrations\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     43\u001b[39m     get_reporting_integration_callbacks,\n\u001b[32m     44\u001b[39m )\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# ruff: isort: on\u001b[39;00m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhuggingface_hub\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhf_hub_utils\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1412\u001b[39m, in \u001b[36m_handle_fromlist\u001b[39m\u001b[34m(module, fromlist, import_, recursive)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\transformers\\utils\\import_utils.py:2317\u001b[39m, in \u001b[36m_LazyModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   2315\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._class_to_module:\n\u001b[32m   2316\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2317\u001b[39m         module = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2318\u001b[39m         value = \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[32m   2319\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\transformers\\utils\\import_utils.py:2347\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   2345\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib.import_module(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m + module_name, \u001b[38;5;28mself\u001b[39m.\u001b[34m__name__\u001b[39m)\n\u001b[32m   2346\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m2347\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\transformers\\utils\\import_utils.py:2345\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   2343\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m   2344\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2345\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2346\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   2347\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python313\\Lib\\importlib\\__init__.py:88\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     86\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     87\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\transformers\\integrations\\integration_utils.py:60\u001b[39m\n\u001b[32m     57\u001b[39m logger = logging.get_logger(\u001b[34m__name__\u001b[39m)\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_tf_available():\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TFPreTrainedModel\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_torch_available():\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen importlib._bootstrap>:1412\u001b[39m, in \u001b[36m_handle_fromlist\u001b[39m\u001b[34m(module, fromlist, import_, recursive)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\transformers\\utils\\import_utils.py:2317\u001b[39m, in \u001b[36m_LazyModule.__getattr__\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m   2315\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._class_to_module:\n\u001b[32m   2316\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2317\u001b[39m         module = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2318\u001b[39m         value = \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[32m   2319\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\transformers\\utils\\import_utils.py:2347\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   2345\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib.import_module(\u001b[33m\"\u001b[39m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m + module_name, \u001b[38;5;28mself\u001b[39m.\u001b[34m__name__\u001b[39m)\n\u001b[32m   2346\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m2347\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\transformers\\utils\\import_utils.py:2345\u001b[39m, in \u001b[36m_LazyModule._get_module\u001b[39m\u001b[34m(self, module_name)\u001b[39m\n\u001b[32m   2343\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module_name: \u001b[38;5;28mstr\u001b[39m):\n\u001b[32m   2344\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2345\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[43m.\u001b[49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[34;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   2346\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   2347\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m e\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python313\\Lib\\importlib\\__init__.py:88\u001b[39m, in \u001b[36mimport_module\u001b[39m\u001b[34m(name, package)\u001b[39m\n\u001b[32m     86\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m     87\u001b[39m         level += \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\transformers\\modeling_tf_utils.py:38\u001b[39m\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpackaging\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mversion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m parse\n\u001b[32m     37\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DataCollatorWithPadding, DefaultDataCollator\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mactivations_tf\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_tf_activation\n\u001b[32m     39\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mconfiguration_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m PretrainedConfig\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdynamic_module_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m custom_object_save\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\transformers\\activations_tf.py:27\u001b[39m\n\u001b[32m     24\u001b[39m     \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mkeras\u001b[39;00m\n\u001b[32m     26\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m parse(keras.__version__).major > \u001b[32m2\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     28\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mYour currently installed version of Keras is Keras 3, but this is not yet supported in \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     29\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mTransformers. Please install the backwards-compatible tf-keras package with \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     30\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m`pip install tf-keras`.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     31\u001b[39m         )\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_gelu\u001b[39m(x):\n\u001b[32m     35\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     36\u001b[39m \u001b[33;03m    Gaussian Error Linear Unit. Original Implementation of the gelu activation function in Google Bert repo when\u001b[39;00m\n\u001b[32m     37\u001b[39m \u001b[33;03m    initially created. For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\u001b[39;00m\n\u001b[32m     38\u001b[39m \u001b[33;03m    0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3)))) Also see\u001b[39;00m\n\u001b[32m     39\u001b[39m \u001b[33;03m    https://huggingface.co/papers/1606.08415\u001b[39;00m\n\u001b[32m     40\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: Your currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`."
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer, AutoModelForQuestionAnswering, AutoModelForCausalLM,\n",
    "    TrainingArguments, Trainer, DefaultDataCollator,\n",
    "    pipeline, set_seed\n",
    ")\n",
    "from transformers.data.data_collator import DataCollatorForLanguageModeling\n",
    "import evaluate\n",
    "from tqdm.auto import tqdm\n",
    "import json\n",
    "from collections import Counter\n",
    "import time\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "set_seed(42)\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 1. Data Preparation (15%)\n",
    "\n",
    "Loading and exploring the SQuAD 2.0 dataset, which contains over 100,000 question-answer pairs from Wikipedia passages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Load SQuAD 2.0 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SQuAD 2.0 dataset\n",
    "print(\"Loading SQuAD 2.0 dataset...\")\n",
    "dataset = load_dataset(\"squad_v2\")\n",
    "\n",
    "print(f\"\\nDataset structure:\")\n",
    "print(dataset)\n",
    "print(f\"\\nTraining samples: {len(dataset['train'])}\")\n",
    "print(f\"Validation samples: {len(dataset['validation'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Explore the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display 5 sample questions/passages/answers\n",
    "print(\"=\" * 100)\n",
    "print(\"SAMPLE QUESTIONS, PASSAGES, AND ANSWERS\")\n",
    "print(\"=\" * 100)\n",
    "\n",
    "for i in range(5):\n",
    "    sample = dataset['train'][i]\n",
    "    print(f\"\\n{'='*100}\")\n",
    "    print(f\"Sample {i+1}:\")\n",
    "    print(f\"{'='*100}\")\n",
    "    print(f\"\\nQuestion: {sample['question']}\")\n",
    "    print(f\"\\nContext (first 300 chars): {sample['context'][:300]}...\")\n",
    "    print(f\"\\nAnswers: {sample['answers']}\")\n",
    "    print(f\"Is Impossible (Unanswerable): {sample['answers']['text'] == []}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute statistics\n",
    "def analyze_dataset(dataset_split):\n",
    "    answerable = 0\n",
    "    unanswerable = 0\n",
    "    passage_lengths = []\n",
    "    question_lengths = []\n",
    "    all_questions = []\n",
    "    \n",
    "    for sample in tqdm(dataset_split, desc=\"Analyzing dataset\"):\n",
    "        # Check if answerable\n",
    "        if len(sample['answers']['text']) > 0:\n",
    "            answerable += 1\n",
    "        else:\n",
    "            unanswerable += 1\n",
    "        \n",
    "        # Collect lengths\n",
    "        passage_lengths.append(len(sample['context'].split()))\n",
    "        question_lengths.append(len(sample['question'].split()))\n",
    "        all_questions.append(sample['question'])\n",
    "    \n",
    "    return {\n",
    "        'answerable': answerable,\n",
    "        'unanswerable': unanswerable,\n",
    "        'passage_lengths': passage_lengths,\n",
    "        'question_lengths': question_lengths,\n",
    "        'all_questions': all_questions\n",
    "    }\n",
    "\n",
    "train_stats = analyze_dataset(dataset['train'])\n",
    "val_stats = analyze_dataset(dataset['validation'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DATASET STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nTraining Set:\")\n",
    "print(f\"  Answerable questions: {train_stats['answerable']} ({train_stats['answerable']/len(dataset['train'])*100:.2f}%)\")\n",
    "print(f\"  Unanswerable questions: {train_stats['unanswerable']} ({train_stats['unanswerable']/len(dataset['train'])*100:.2f}%)\")\n",
    "print(f\"  Average passage length: {np.mean(train_stats['passage_lengths']):.2f} words\")\n",
    "print(f\"  Average question length: {np.mean(train_stats['question_lengths']):.2f} words\")\n",
    "\n",
    "print(f\"\\nValidation Set:\")\n",
    "print(f\"  Answerable questions: {val_stats['answerable']} ({val_stats['answerable']/len(dataset['validation'])*100:.2f}%)\")\n",
    "print(f\"  Unanswerable questions: {val_stats['unanswerable']} ({val_stats['unanswerable']/len(dataset['validation'])*100:.2f}%)\")\n",
    "print(f\"  Average passage length: {np.mean(val_stats['passage_lengths']):.2f} words\")\n",
    "print(f\"  Average question length: {np.mean(val_stats['question_lengths']):.2f} words\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualizations\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "# 1. Passage length histogram\n",
    "axes[0, 0].hist(train_stats['passage_lengths'], bins=50, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].set_xlabel('Passage Length (words)', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0, 0].set_title('Distribution of Passage Lengths', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].axvline(np.mean(train_stats['passage_lengths']), color='red', linestyle='--', \n",
    "                   label=f\"Mean: {np.mean(train_stats['passage_lengths']):.1f}\")\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(alpha=0.3)\n",
    "\n",
    "# 2. Question length histogram\n",
    "axes[0, 1].hist(train_stats['question_lengths'], bins=30, color='lightcoral', edgecolor='black', alpha=0.7)\n",
    "axes[0, 1].set_xlabel('Question Length (words)', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Frequency', fontsize=12)\n",
    "axes[0, 1].set_title('Distribution of Question Lengths', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].axvline(np.mean(train_stats['question_lengths']), color='red', linestyle='--',\n",
    "                   label=f\"Mean: {np.mean(train_stats['question_lengths']):.1f}\")\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(alpha=0.3)\n",
    "\n",
    "# 3. Answerable vs Unanswerable pie chart\n",
    "labels = ['Answerable', 'Unanswerable']\n",
    "sizes = [train_stats['answerable'], train_stats['unanswerable']]\n",
    "colors = ['#66b3ff', '#ff9999']\n",
    "explode = (0.05, 0)\n",
    "\n",
    "axes[1, 0].pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%',\n",
    "               shadow=True, startangle=90, textprops={'fontsize': 12})\n",
    "axes[1, 0].set_title('Answerable vs Unanswerable Questions', fontsize=14, fontweight='bold')\n",
    "\n",
    "# 4. Word cloud for questions\n",
    "all_questions_text = ' '.join(train_stats['all_questions'][:5000])  # Use subset for speed\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white', \n",
    "                      colormap='viridis', max_words=100).generate(all_questions_text)\n",
    "\n",
    "axes[1, 1].imshow(wordcloud, interpolation='bilinear')\n",
    "axes[1, 1].axis('off')\n",
    "axes[1, 1].set_title('Word Cloud of Questions', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('data_exploration.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n Visualizations saved as 'data_exploration.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4 Data Preprocessing for Extractive QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer for BERT\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "print(f\"Loaded tokenizer: {model_checkpoint}\")\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing function for extractive QA\n",
    "max_length = 384  # Maximum length for passages\n",
    "doc_stride = 128  # Stride for splitting long contexts\n",
    "\n",
    "def preprocess_training_examples(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_second\",\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        sample_idx = sample_map[i]\n",
    "        answer = answers[sample_idx]\n",
    "        \n",
    "        # If no answers are given, set the cls_index as answer\n",
    "        if len(answer[\"answer_start\"]) == 0:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            start_char = answer[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answer[\"text\"][0])\n",
    "            sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "            # Find the start and end of the context\n",
    "            idx = 0\n",
    "            while sequence_ids[idx] != 1:\n",
    "                idx += 1\n",
    "            context_start = idx\n",
    "            while sequence_ids[idx] == 1:\n",
    "                idx += 1\n",
    "            context_end = idx - 1\n",
    "\n",
    "            # If the answer is not fully inside the context, label is (0, 0)\n",
    "            if offset[context_start][0] > start_char or offset[context_end][1] < end_char:\n",
    "                start_positions.append(0)\n",
    "                end_positions.append(0)\n",
    "            else:\n",
    "                # Otherwise it's the start and end token positions\n",
    "                idx = context_start\n",
    "                while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                    idx += 1\n",
    "                start_positions.append(idx - 1)\n",
    "\n",
    "                idx = context_end\n",
    "                while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                    idx -= 1\n",
    "                end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess validation examples (slightly different for evaluation)\n",
    "def preprocess_validation_examples(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_second\",\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    sample_map = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "    example_ids = []\n",
    "\n",
    "    for i in range(len(inputs[\"input_ids\"])):\n",
    "        sample_idx = sample_map[i]\n",
    "        example_ids.append(examples[\"id\"][sample_idx])\n",
    "\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        offset = inputs[\"offset_mapping\"][i]\n",
    "        inputs[\"offset_mapping\"][i] = [\n",
    "            o if sequence_ids[k] == 1 else None for k, o in enumerate(offset)\n",
    "        ]\n",
    "\n",
    "    inputs[\"example_id\"] = example_ids\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply preprocessing - use subset for faster training (optional)\n",
    "# For full dataset, remove the .select() calls\n",
    "USE_SUBSET = True  # Set to False for full dataset\n",
    "\n",
    "if USE_SUBSET:\n",
    "    train_dataset = dataset[\"train\"].select(range(10000))  # Use 10k samples\n",
    "    val_dataset = dataset[\"validation\"].select(range(1000))  # Use 1k samples\n",
    "    print(\"Using subset of data for faster training\")\n",
    "else:\n",
    "    train_dataset = dataset[\"train\"]\n",
    "    val_dataset = dataset[\"validation\"]\n",
    "    print(\"Using full dataset\")\n",
    "\n",
    "print(f\"\\nPreprocessing training data...\")\n",
    "tokenized_train = train_dataset.map(\n",
    "    preprocess_training_examples,\n",
    "    batched=True,\n",
    "    remove_columns=train_dataset.column_names,\n",
    "    desc=\"Tokenizing training set\"\n",
    ")\n",
    "\n",
    "print(f\"Preprocessing validation data...\")\n",
    "tokenized_val = val_dataset.map(\n",
    "    preprocess_validation_examples,\n",
    "    batched=True,\n",
    "    remove_columns=val_dataset.column_names,\n",
    "    desc=\"Tokenizing validation set\"\n",
    ")\n",
    "\n",
    "print(f\"\\n Preprocessing complete!\")\n",
    "print(f\"  Training samples: {len(tokenized_train)}\")\n",
    "print(f\"  Validation samples: {len(tokenized_val)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 2. Extractive QA with Transformers (25%)\n",
    "\n",
    "Fine-tuning DistilBERT for extractive question answering to predict answer spans in passages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Load Pre-trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained DistilBERT for QA\n",
    "from transformers import AutoModelForQuestionAnswering\n",
    "\n",
    "qa_model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
    "qa_model.to(device)\n",
    "\n",
    "print(f\"Model loaded: {model_checkpoint}\")\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in qa_model.parameters()):,}\")\n",
    "print(f\"Trainable parameters: {sum(p.numel() for p in qa_model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Setup Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training arguments\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qa_model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    save_total_limit=2,\n",
    "    fp16=torch.cuda.is_available(),  # Use mixed precision if GPU available\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  Mixed precision (FP16): {training_args.fp16}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator\n",
    "from transformers import DefaultDataCollator\n",
    "\n",
    "data_collator = DefaultDataCollator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Trainer\n",
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=qa_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_val,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized. Starting training...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "start_time = time.time()\n",
    "train_result = trainer.train()\n",
    "training_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n Training completed in {training_time/60:.2f} minutes\")\n",
    "print(f\"  Final training loss: {train_result.training_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "trainer.save_model(\"./qa_model_final\")\n",
    "tokenizer.save_pretrained(\"./qa_model_final\")\n",
    "print(\" Model saved to './qa_model_final'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4 Plot Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract training history\n",
    "import pandas as pd\n",
    "\n",
    "log_history = trainer.state.log_history\n",
    "\n",
    "# Separate training and validation logs\n",
    "train_logs = [log for log in log_history if 'loss' in log and 'eval_loss' not in log]\n",
    "eval_logs = [log for log in log_history if 'eval_loss' in log]\n",
    "\n",
    "# Create DataFrame\n",
    "train_df = pd.DataFrame(train_logs)\n",
    "eval_df = pd.DataFrame(eval_logs)\n",
    "\n",
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Training loss\n",
    "if 'step' in train_df.columns and 'loss' in train_df.columns:\n",
    "    axes[0].plot(train_df['step'], train_df['loss'], marker='o', linewidth=2, markersize=4)\n",
    "    axes[0].set_xlabel('Training Steps', fontsize=12)\n",
    "    axes[0].set_ylabel('Loss', fontsize=12)\n",
    "    axes[0].set_title('Training Loss Over Time', fontsize=14, fontweight='bold')\n",
    "    axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Validation loss\n",
    "if 'epoch' in eval_df.columns and 'eval_loss' in eval_df.columns:\n",
    "    axes[1].plot(eval_df['epoch'], eval_df['eval_loss'], marker='s', color='orange', \n",
    "                 linewidth=2, markersize=8, label='Validation Loss')\n",
    "    axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[1].set_ylabel('Loss', fontsize=12)\n",
    "    axes[1].set_title('Validation Loss per Epoch', fontsize=14, fontweight='bold')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('qa_training_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\" Training curves saved as 'qa_training_curves.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Evaluation with Exact Match and F1 Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Post-processing function to extract answers\n",
    "import collections\n",
    "\n",
    "def postprocess_qa_predictions(examples, features, raw_predictions, n_best_size=20, max_answer_length=30):\n",
    "    all_start_logits, all_end_logits = raw_predictions\n",
    "    \n",
    "    # Build a map example to its corresponding features\n",
    "    example_id_to_index = {k: i for i, k in enumerate(examples[\"id\"])}\n",
    "    features_per_example = collections.defaultdict(list)\n",
    "    for i, feature in enumerate(features):\n",
    "        features_per_example[example_id_to_index[feature[\"example_id\"]]].append(i)\n",
    "\n",
    "    predictions = collections.OrderedDict()\n",
    "\n",
    "    for example_index, example in enumerate(tqdm(examples, desc=\"Post-processing\")):\n",
    "        feature_indices = features_per_example[example_index]\n",
    "        \n",
    "        min_null_score = None\n",
    "        valid_answers = []\n",
    "        \n",
    "        context = example[\"context\"]\n",
    "        \n",
    "        for feature_index in feature_indices:\n",
    "            start_logits = all_start_logits[feature_index]\n",
    "            end_logits = all_end_logits[feature_index]\n",
    "            offset_mapping = features[feature_index][\"offset_mapping\"]\n",
    "            \n",
    "            cls_index = 0\n",
    "            feature_null_score = start_logits[cls_index] + end_logits[cls_index]\n",
    "            if min_null_score is None or min_null_score < feature_null_score:\n",
    "                min_null_score = feature_null_score\n",
    "            \n",
    "            start_indexes = np.argsort(start_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            end_indexes = np.argsort(end_logits)[-1 : -n_best_size - 1 : -1].tolist()\n",
    "            \n",
    "            for start_index in start_indexes:\n",
    "                for end_index in end_indexes:\n",
    "                    if (\n",
    "                        start_index >= len(offset_mapping)\n",
    "                        or end_index >= len(offset_mapping)\n",
    "                        or offset_mapping[start_index] is None\n",
    "                        or offset_mapping[end_index] is None\n",
    "                    ):\n",
    "                        continue\n",
    "                    \n",
    "                    if end_index < start_index or end_index - start_index + 1 > max_answer_length:\n",
    "                        continue\n",
    "                    \n",
    "                    start_char = offset_mapping[start_index][0]\n",
    "                    end_char = offset_mapping[end_index][1]\n",
    "                    valid_answers.append(\n",
    "                        {\n",
    "                            \"score\": start_logits[start_index] + end_logits[end_index],\n",
    "                            \"text\": context[start_char: end_char]\n",
    "                        }\n",
    "                    )\n",
    "        \n",
    "        if len(valid_answers) > 0:\n",
    "            best_answer = sorted(valid_answers, key=lambda x: x[\"score\"], reverse=True)[0]\n",
    "        else:\n",
    "            best_answer = {\"text\": \"\", \"score\": 0.0}\n",
    "        \n",
    "        # Compare null score with best answer score\n",
    "        if min_null_score is not None and min_null_score > best_answer[\"score\"]:\n",
    "            predictions[example[\"id\"]] = \"\"\n",
    "        else:\n",
    "            predictions[example[\"id\"]] = best_answer[\"text\"]\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "print(\"Generating predictions on validation set...\")\n",
    "raw_predictions = trainer.predict(tokenized_val)\n",
    "\n",
    "final_predictions = postprocess_qa_predictions(\n",
    "    val_dataset,\n",
    "    tokenized_val,\n",
    "    raw_predictions.predictions\n",
    ")\n",
    "\n",
    "print(f\" Generated {len(final_predictions)} predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute metrics\n",
    "metric = evaluate.load(\"squad_v2\")\n",
    "\n",
    "# Format references\n",
    "theoretical_answers = [\n",
    "    {\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in val_dataset\n",
    "]\n",
    "\n",
    "# Compute metrics\n",
    "results = metric.compute(predictions=final_predictions, references=theoretical_answers)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXTRACTIVE QA EVALUATION RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Exact Match (EM): {results['exact']:.2f}%\")\n",
    "print(f\"F1 Score: {results['f1']:.2f}%\")\n",
    "print(f\"HasAns Exact Match: {results.get('HasAns_exact', 0):.2f}%\")\n",
    "print(f\"HasAns F1: {results.get('HasAns_f1', 0):.2f}%\")\n",
    "print(f\"NoAns Exact Match: {results.get('NoAns_exact', 0):.2f}%\")\n",
    "print(f\"NoAns F1: {results.get('NoAns_f1', 0):.2f}%\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample predictions\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"SAMPLE PREDICTIONS\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "for i in range(5):\n",
    "    example = val_dataset[i]\n",
    "    pred_answer = final_predictions[example[\"id\"]]\n",
    "    true_answer = example[\"answers\"][\"text\"][0] if len(example[\"answers\"][\"text\"]) > 0 else \"[No Answer]\"\n",
    "    \n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Question: {example['question']}\")\n",
    "    print(f\"Context (first 200 chars): {example['context'][:200]}...\")\n",
    "    print(f\"Predicted Answer: {pred_answer if pred_answer else '[No Answer]'}\")\n",
    "    print(f\"True Answer: {true_answer}\")\n",
    "    print(f\"Match: {'' if pred_answer == true_answer else ''}\")\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 3. Response Generation with Transformers (25%)\n",
    "\n",
    "Fine-tuning GPT-2 to generate explanatory responses based on extracted answers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Prepare Data for Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create prompt-response pairs for generation\n",
    "def create_generation_dataset(dataset_split, max_samples=5000):\n",
    "    prompts = []\n",
    "    responses = []\n",
    "    \n",
    "    for i, example in enumerate(tqdm(dataset_split, desc=\"Creating generation dataset\")):\n",
    "        if i >= max_samples:\n",
    "            break\n",
    "            \n",
    "        # Only use answerable questions\n",
    "        if len(example['answers']['text']) == 0:\n",
    "            continue\n",
    "        \n",
    "        question = example['question']\n",
    "        context = example['context'][:300]  # Truncate context for generation\n",
    "        answer = example['answers']['text'][0]\n",
    "        \n",
    "        # Create prompt\n",
    "        prompt = f\"Question: {question}\\nContext: {context}\\nAnswer: {answer}\\nExplanation:\"\n",
    "        \n",
    "        # Create response (simple template - in practice, you'd want better explanations)\n",
    "        response = f\"Based on the context, the answer to '{question}' is '{answer}'. This can be found in the passage where it states relevant information about the question.\"\n",
    "        \n",
    "        prompts.append(prompt)\n",
    "        responses.append(response)\n",
    "    \n",
    "    return prompts, responses\n",
    "\n",
    "print(\"Creating generation training data...\")\n",
    "train_prompts, train_responses = create_generation_dataset(dataset['train'], max_samples=5000)\n",
    "\n",
    "print(\"Creating generation validation data...\")\n",
    "val_prompts, val_responses = create_generation_dataset(dataset['validation'], max_samples=500)\n",
    "\n",
    "print(f\"\\n Generation dataset created\")\n",
    "print(f\"  Training samples: {len(train_prompts)}\")\n",
    "print(f\"  Validation samples: {len(val_prompts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample prompt-response pairs\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"SAMPLE PROMPT-RESPONSE PAIRS\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "for i in range(2):\n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    print(f\"\\nPrompt:\\n{train_prompts[i]}\")\n",
    "    print(f\"\\nResponse:\\n{train_responses[i]}\")\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Prepare GPT-2 for Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GPT-2 tokenizer and model\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "gpt2_model_name = \"distilgpt2\"\n",
    "gpt2_tokenizer = AutoTokenizer.from_pretrained(gpt2_model_name)\n",
    "gpt2_model = AutoModelForCausalLM.from_pretrained(gpt2_model_name)\n",
    "\n",
    "# Set pad token (GPT-2 doesn't have one by default)\n",
    "gpt2_tokenizer.pad_token = gpt2_tokenizer.eos_token\n",
    "gpt2_model.config.pad_token_id = gpt2_tokenizer.eos_token_id\n",
    "\n",
    "gpt2_model.to(device)\n",
    "\n",
    "print(f\"Model loaded: {gpt2_model_name}\")\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in gpt2_model.parameters()):,}\")\n",
    "print(f\"Vocabulary size: {gpt2_tokenizer.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize generation data\n",
    "def tokenize_generation_data(prompts, responses, tokenizer, max_length=512):\n",
    "    # Combine prompt and response\n",
    "    full_texts = [p + \" \" + r + tokenizer.eos_token for p, r in zip(prompts, responses)]\n",
    "    \n",
    "    # Tokenize\n",
    "    encodings = tokenizer(\n",
    "        full_texts,\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    return encodings\n",
    "\n",
    "print(\"Tokenizing generation data...\")\n",
    "train_encodings = tokenize_generation_data(train_prompts, train_responses, gpt2_tokenizer)\n",
    "val_encodings = tokenize_generation_data(val_prompts, val_responses, gpt2_tokenizer)\n",
    "\n",
    "print(\" Tokenization complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PyTorch datasets\n",
    "class GenerationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, encodings):\n",
    "        self.encodings = encodings\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
    "        item['labels'] = item['input_ids'].clone()\n",
    "        return item\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "train_gen_dataset = GenerationDataset(train_encodings)\n",
    "val_gen_dataset = GenerationDataset(val_encodings)\n",
    "\n",
    "print(f\"Dataset created with {len(train_gen_dataset)} training and {len(val_gen_dataset)} validation samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Train GPT-2 for Response Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments for GPT-2\n",
    "gen_training_args = TrainingArguments(\n",
    "    output_dir=\"./gpt2_generation\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    push_to_hub=False,\n",
    "    logging_dir=\"./logs_gen\",\n",
    "    logging_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    save_total_limit=2,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "print(\"GPT-2 Training configuration:\")\n",
    "print(f\"  Batch size: {gen_training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Learning rate: {gen_training_args.learning_rate}\")\n",
    "print(f\"  Epochs: {gen_training_args.num_train_epochs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainer for GPT-2\n",
    "gen_trainer = Trainer(\n",
    "    model=gpt2_model,\n",
    "    args=gen_training_args,\n",
    "    train_dataset=train_gen_dataset,\n",
    "    eval_dataset=val_gen_dataset,\n",
    "    tokenizer=gpt2_tokenizer,\n",
    ")\n",
    "\n",
    "print(\"Trainer initialized for GPT-2. Starting training...\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train GPT-2\n",
    "start_time = time.time()\n",
    "gen_train_result = gen_trainer.train()\n",
    "gen_training_time = time.time() - start_time\n",
    "\n",
    "print(f\"\\n GPT-2 training completed in {gen_training_time/60:.2f} minutes\")\n",
    "print(f\"  Final training loss: {gen_train_result.training_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save GPT-2 model\n",
    "gen_trainer.save_model(\"./gpt2_generation_final\")\n",
    "gpt2_tokenizer.save_pretrained(\"./gpt2_generation_final\")\n",
    "print(\" GPT-2 model saved to './gpt2_generation_final'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Plot Training Curves for GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract GPT-2 training history\n",
    "gen_log_history = gen_trainer.state.log_history\n",
    "\n",
    "gen_train_logs = [log for log in gen_log_history if 'loss' in log and 'eval_loss' not in log]\n",
    "gen_eval_logs = [log for log in gen_log_history if 'eval_loss' in log]\n",
    "\n",
    "gen_train_df = pd.DataFrame(gen_train_logs)\n",
    "gen_eval_df = pd.DataFrame(gen_eval_logs)\n",
    "\n",
    "# Plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "if 'step' in gen_train_df.columns and 'loss' in gen_train_df.columns:\n",
    "    axes[0].plot(gen_train_df['step'], gen_train_df['loss'], marker='o', linewidth=2, markersize=4, color='green')\n",
    "    axes[0].set_xlabel('Training Steps', fontsize=12)\n",
    "    axes[0].set_ylabel('Loss', fontsize=12)\n",
    "    axes[0].set_title('GPT-2 Training Loss Over Time', fontsize=14, fontweight='bold')\n",
    "    axes[0].grid(alpha=0.3)\n",
    "\n",
    "if 'epoch' in gen_eval_df.columns and 'eval_loss' in gen_eval_df.columns:\n",
    "    axes[1].plot(gen_eval_df['epoch'], gen_eval_df['eval_loss'], marker='s', color='red', \n",
    "                 linewidth=2, markersize=8, label='Validation Loss')\n",
    "    axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[1].set_ylabel('Loss', fontsize=12)\n",
    "    axes[1].set_title('GPT-2 Validation Loss per Epoch', fontsize=14, fontweight='bold')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('gpt2_training_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\" GPT-2 training curves saved as 'gpt2_training_curves.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Generate Sample Explanations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate explanations for sample QA pairs\n",
    "def generate_explanation(question, context, answer, model, tokenizer, max_length=150):\n",
    "    prompt = f\"Question: {question}\\nContext: {context[:300]}\\nAnswer: {answer}\\nExplanation:\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=400).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=inputs['input_ids'].shape[1] + max_length,\n",
    "            num_return_sequences=1,\n",
    "            temperature=0.7,\n",
    "            top_p=0.9,\n",
    "            do_sample=True,\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Extract only the explanation part\n",
    "    if \"Explanation:\" in generated_text:\n",
    "        explanation = generated_text.split(\"Explanation:\")[1].strip()\n",
    "    else:\n",
    "        explanation = generated_text\n",
    "    \n",
    "    return explanation\n",
    "\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"GENERATED EXPLANATIONS\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "# Generate for 5 samples\n",
    "for i in range(5):\n",
    "    example = dataset['validation'][i]\n",
    "    \n",
    "    # Skip unanswerable questions\n",
    "    if len(example['answers']['text']) == 0:\n",
    "        continue\n",
    "    \n",
    "    question = example['question']\n",
    "    context = example['context']\n",
    "    answer = example['answers']['text'][0]\n",
    "    \n",
    "    explanation = generate_explanation(question, context, answer, gpt2_model, gpt2_tokenizer)\n",
    "    \n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"Answer: {answer}\")\n",
    "    print(f\"Generated Explanation: {explanation}\")\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6 Evaluate with Perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate perplexity on validation set\n",
    "def calculate_perplexity(model, dataset, tokenizer, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(len(dataset)), desc=\"Calculating perplexity\"):\n",
    "            item = dataset[i]\n",
    "            input_ids = item['input_ids'].unsqueeze(0).to(device)\n",
    "            labels = item['labels'].unsqueeze(0).to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, labels=labels)\n",
    "            loss = outputs.loss\n",
    "            \n",
    "            # Count non-padding tokens\n",
    "            non_pad_tokens = (labels != tokenizer.pad_token_id).sum().item()\n",
    "            \n",
    "            total_loss += loss.item() * non_pad_tokens\n",
    "            total_tokens += non_pad_tokens\n",
    "    \n",
    "    avg_loss = total_loss / total_tokens\n",
    "    perplexity = torch.exp(torch.tensor(avg_loss)).item()\n",
    "    \n",
    "    return perplexity\n",
    "\n",
    "print(\"Calculating perplexity on validation set...\")\n",
    "perplexity = calculate_perplexity(gpt2_model, val_gen_dataset, gpt2_tokenizer, device)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"GENERATION MODEL EVALUATION\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Perplexity on validation set: {perplexity:.2f}\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 4. Advanced Exploration (15%)\n",
    "\n",
    "Attention visualization, zero-shot comparison, and parameter-efficient fine-tuning experiments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Attention Visualization with BertViz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install bertviz if needed\n",
    "try:\n",
    "    from bertviz import head_view, model_view\n",
    "except ImportError:\n",
    "    !pip install bertviz\n",
    "    from bertviz import head_view, model_view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention for sample QA pairs\n",
    "from bertviz import head_view\n",
    "\n",
    "def visualize_attention(question, context, model, tokenizer):\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(question, context, return_tensors=\"pt\", truncation=True, max_length=384)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Get model outputs with attentions\n",
    "    outputs = model(**inputs, output_attentions=True)\n",
    "    \n",
    "    # Get tokens\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "    \n",
    "    # Get attention\n",
    "    attention = outputs.attentions\n",
    "    \n",
    "    # Visualize\n",
    "    return head_view(attention, tokens)\n",
    "\n",
    "print(\"Visualizing attention for sample QA pairs...\\n\")\n",
    "\n",
    "# Select 2 examples for visualization\n",
    "for i in range(2):\n",
    "    example = val_dataset[i]\n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Question: {example['question']}\")\n",
    "    print(f\"Context (first 200 chars): {example['context'][:200]}...\\n\")\n",
    "    \n",
    "    # Visualize (this will display interactive visualization in Jupyter)\n",
    "    visualize_attention(example['question'], example['context'][:300], qa_model, tokenizer)\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Create static attention heatmap\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def plot_attention_heatmap(question, context, model, tokenizer, layer=5, head=0):\n",
    "    # Tokenize\n",
    "    inputs = tokenizer(question, context, return_tensors=\"pt\", truncation=True, max_length=128)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Get attention\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs, output_attentions=True)\n",
    "    \n",
    "    # Get tokens\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "    \n",
    "    # Get attention for specific layer and head\n",
    "    attention = outputs.attentions[layer][0, head].cpu().numpy()\n",
    "    \n",
    "    # Plot\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    sns.heatmap(attention, xticklabels=tokens, yticklabels=tokens, cmap='viridis', ax=ax, cbar=True)\n",
    "    ax.set_title(f'Attention Heatmap - Layer {layer}, Head {head}', fontsize=14, fontweight='bold')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# Plot for one example\n",
    "example = val_dataset[0]\n",
    "fig = plot_attention_heatmap(example['question'], example['context'][:200], qa_model, tokenizer)\n",
    "plt.savefig('attention_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\" Attention heatmap saved as 'attention_heatmap.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Zero-Shot vs Fine-Tuned Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained (zero-shot) model\n",
    "print(\"Loading pre-trained model for zero-shot comparison...\")\n",
    "zero_shot_model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
    "zero_shot_model.to(device)\n",
    "zero_shot_model.eval()\n",
    "\n",
    "print(\" Zero-shot model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate zero-shot model on subset\n",
    "from transformers import pipeline\n",
    "\n",
    "# Create pipelines\n",
    "zero_shot_qa = pipeline(\"question-answering\", model=zero_shot_model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)\n",
    "fine_tuned_qa = pipeline(\"question-answering\", model=qa_model, tokenizer=tokenizer, device=0 if torch.cuda.is_available() else -1)\n",
    "\n",
    "print(\"Comparing zero-shot vs fine-tuned performance...\\n\")\n",
    "\n",
    "# Test on subset\n",
    "test_samples = 100\n",
    "zero_shot_correct = 0\n",
    "fine_tuned_correct = 0\n",
    "\n",
    "for i in tqdm(range(test_samples), desc=\"Evaluating\"):\n",
    "    example = val_dataset[i]\n",
    "    \n",
    "    # Skip unanswerable\n",
    "    if len(example['answers']['text']) == 0:\n",
    "        continue\n",
    "    \n",
    "    question = example['question']\n",
    "    context = example['context']\n",
    "    true_answer = example['answers']['text'][0]\n",
    "    \n",
    "    # Zero-shot prediction\n",
    "    try:\n",
    "        zero_shot_pred = zero_shot_qa(question=question, context=context)\n",
    "        if zero_shot_pred['answer'].lower() == true_answer.lower():\n",
    "            zero_shot_correct += 1\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # Fine-tuned prediction\n",
    "    try:\n",
    "        fine_tuned_pred = fine_tuned_qa(question=question, context=context)\n",
    "        if fine_tuned_pred['answer'].lower() == true_answer.lower():\n",
    "            fine_tuned_correct += 1\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"ZERO-SHOT VS FINE-TUNED COMPARISON\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Zero-shot accuracy: {zero_shot_correct/test_samples*100:.2f}%\")\n",
    "print(f\"Fine-tuned accuracy: {fine_tuned_correct/test_samples*100:.2f}%\")\n",
    "print(f\"Improvement: {(fine_tuned_correct - zero_shot_correct)/test_samples*100:.2f}%\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show side-by-side comparison examples\n",
    "print(\"\\n\" + \"=\"*100)\n",
    "print(\"SIDE-BY-SIDE COMPARISON EXAMPLES\")\n",
    "print(\"=\"*100)\n",
    "\n",
    "for i in range(3):\n",
    "    example = val_dataset[i]\n",
    "    \n",
    "    if len(example['answers']['text']) == 0:\n",
    "        continue\n",
    "    \n",
    "    question = example['question']\n",
    "    context = example['context'][:400]\n",
    "    true_answer = example['answers']['text'][0]\n",
    "    \n",
    "    zero_shot_pred = zero_shot_qa(question=question, context=context)\n",
    "    fine_tuned_pred = fine_tuned_qa(question=question, context=context)\n",
    "    \n",
    "    print(f\"\\nExample {i+1}:\")\n",
    "    print(f\"Question: {question}\")\n",
    "    print(f\"True Answer: {true_answer}\")\n",
    "    print(f\"Zero-shot Prediction: {zero_shot_pred['answer']} (confidence: {zero_shot_pred['score']:.3f})\")\n",
    "    print(f\"Fine-tuned Prediction: {fine_tuned_pred['answer']} (confidence: {fine_tuned_pred['score']:.3f})\")\n",
    "    print(\"-\" * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Parameter-Efficient Fine-Tuning with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install PEFT library for LoRA\n",
    "try:\n",
    "    from peft import LoraConfig, get_peft_model, TaskType\n",
    "except ImportError:\n",
    "    !pip install peft\n",
    "    from peft import LoraConfig, get_peft_model, TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup LoRA configuration\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# Load fresh model for LoRA\n",
    "lora_model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)\n",
    "\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.QUESTION_ANS,\n",
    "    r=8,  # LoRA rank\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_lin\", \"v_lin\"],  # For DistilBERT\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "lora_model = get_peft_model(lora_model, lora_config)\n",
    "lora_model.to(device)\n",
    "\n",
    "# Print trainable parameters\n",
    "lora_model.print_trainable_parameters()\n",
    "\n",
    "total_params = sum(p.numel() for p in lora_model.parameters())\n",
    "trainable_params = sum(p.numel() for p in lora_model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters with LoRA: {trainable_params:,}\")\n",
    "print(f\"Percentage of trainable parameters: {trainable_params/total_params*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train with LoRA (shorter training for comparison)\n",
    "lora_training_args = TrainingArguments(\n",
    "    output_dir=\"./qa_model_lora\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=3e-4,  # Higher LR for LoRA\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=2,  # Fewer epochs for demo\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=100,\n",
    "    save_strategy=\"no\",  # Don't save checkpoints\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "lora_trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=lora_training_args,\n",
    "    train_dataset=tokenized_train.select(range(5000)),  # Use subset\n",
    "    eval_dataset=tokenized_val.select(range(500)),\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "print(\"Training LoRA model...\\n\")\n",
    "lora_start_time = time.time()\n",
    "lora_train_result = lora_trainer.train()\n",
    "lora_training_time = time.time() - lora_start_time\n",
    "\n",
    "print(f\"\\n LoRA training completed in {lora_training_time/60:.2f} minutes\")\n",
    "print(f\"  Final training loss: {lora_train_result.training_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare efficiency\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EFFICIENCY COMPARISON: Full Fine-Tuning vs LoRA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "full_params = sum(p.numel() for p in qa_model.parameters() if p.requires_grad)\n",
    "lora_params = sum(p.numel() for p in lora_model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\nTrainable Parameters:\")\n",
    "print(f\"  Full Fine-Tuning: {full_params:,}\")\n",
    "print(f\"  LoRA: {lora_params:,}\")\n",
    "print(f\"  Reduction: {(1 - lora_params/full_params)*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nTraining Time (approximate):\")\n",
    "print(f\"  Full Fine-Tuning: {training_time/60:.2f} minutes\")\n",
    "print(f\"  LoRA: {lora_training_time/60:.2f} minutes\")\n",
    "\n",
    "print(f\"\\nPerformance:\")\n",
    "print(f\"  Full Fine-Tuning Loss: {train_result.training_loss:.4f}\")\n",
    "print(f\"  LoRA Loss: {lora_train_result.training_loss:.4f}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 5. Integrated Demo (10%)\n",
    "\n",
    "Building an interactive Gradio interface for the complete QA system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 Create Gradio Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Gradio\n",
    "try:\n",
    "    import gradio as gr\n",
    "except ImportError:\n",
    "    !pip install gradio\n",
    "    import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# Define the QA function\n",
    "def answer_question(passage, question):\n",
    "    \"\"\"\n",
    "    Complete QA pipeline: Extract answer and generate explanation\n",
    "    \"\"\"\n",
    "    if not passage or not question:\n",
    "        return \"Please provide both a passage and a question.\", \"\"\n",
    "    \n",
    "    try:\n",
    "        # Step 1: Extract answer using fine-tuned BERT\n",
    "        qa_pipeline = pipeline(\n",
    "            \"question-answering\",\n",
    "            model=qa_model,\n",
    "            tokenizer=tokenizer,\n",
    "            device=0 if torch.cuda.is_available() else -1\n",
    "        )\n",
    "        \n",
    "        result = qa_pipeline(question=question, context=passage)\n",
    "        extracted_answer = result['answer']\n",
    "        confidence = result['score']\n",
    "        \n",
    "        # Step 2: Generate explanation using fine-tuned GPT-2\n",
    "        explanation = generate_explanation(\n",
    "            question,\n",
    "            passage,\n",
    "            extracted_answer,\n",
    "            gpt2_model,\n",
    "            gpt2_tokenizer,\n",
    "            max_length=100\n",
    "        )\n",
    "        \n",
    "        # Format output\n",
    "        answer_output = f\"**Answer:** {extracted_answer}\\n\\n**Confidence:** {confidence:.2%}\"\n",
    "        explanation_output = f\"**Explanation:** {explanation}\"\n",
    "        \n",
    "        return answer_output, explanation_output\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"Error: {str(e)}\", \"\"\n",
    "\n",
    "# Create Gradio interface\n",
    "demo = gr.Interface(\n",
    "    fn=answer_question,\n",
    "    inputs=[\n",
    "        gr.Textbox(\n",
    "            lines=10,\n",
    "            placeholder=\"Enter the passage/context here...\",\n",
    "            label=\" Passage\"\n",
    "        ),\n",
    "        gr.Textbox(\n",
    "            lines=2,\n",
    "            placeholder=\"Enter your question here...\",\n",
    "            label=\" Question\"\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        gr.Markdown(label=\" Extracted Answer\"),\n",
    "        gr.Markdown(label=\" Generated Explanation\")\n",
    "    ],\n",
    "    title=\" Transformer-Based Question Answering System\",\n",
    "    description=\"\"\"\n",
    "    This system uses two fine-tuned Transformer models:\n",
    "    1. **DistilBERT** for extractive question answering (finding the answer in the passage)\n",
    "    2. **DistilGPT-2** for generating explanatory responses\n",
    "    \n",
    "    Enter a passage and a question to get an extracted answer with an AI-generated explanation!\n",
    "    \"\"\",\n",
    "    examples=[\n",
    "        [\n",
    "            \"The Amazon rainforest, also known as Amazonia, is a moist broadleaf tropical rainforest in the Amazon biome that covers most of the Amazon basin of South America. This basin encompasses 7,000,000 km2, of which 5,500,000 km2 are covered by the rainforest. The majority of the forest is contained within Brazil, with 60% of the rainforest, followed by Peru with 13%, and Colombia with 10%.\",\n",
    "            \"Which country contains the majority of the Amazon rainforest?\"\n",
    "        ],\n",
    "        [\n",
    "            \"The Eiffel Tower is a wrought-iron lattice tower on the Champ de Mars in Paris, France. It is named after the engineer Gustave Eiffel, whose company designed and built the tower. Constructed from 1887 to 1889, it was initially criticized by some of France's leading artists and intellectuals for its design, but it has become a global cultural icon of France.\",\n",
    "            \"When was the Eiffel Tower constructed?\"\n",
    "        ],\n",
    "        [\n",
    "            \"Photosynthesis is a process used by plants and other organisms to convert light energy into chemical energy that can later be released to fuel the organism's activities. This chemical energy is stored in carbohydrate molecules, such as sugars, which are synthesized from carbon dioxide and water.\",\n",
    "            \"What do plants convert light energy into?\"\n",
    "        ]\n",
    "    ],\n",
    "    theme=gr.themes.Soft(),\n",
    "    allow_flagging=\"never\"\n",
    ")\n",
    "\n",
    "print(\" Gradio interface created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 Launch Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch the demo\n",
    "print(\"Launching Gradio demo...\\n\")\n",
    "print(\"The demo will open in a new browser tab.\")\n",
    "print(\"You can also share it publicly by setting share=True\\n\")\n",
    "\n",
    "# Launch locally\n",
    "demo.launch(share=False, debug=True)\n",
    "\n",
    "# To deploy on Hugging Face Spaces:\n",
    "# 1. Create a new Space on huggingface.co\n",
    "# 2. Upload this notebook and models\n",
    "# 3. The Space will automatically run the Gradio interface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.3 Deployment Instructions\n",
    "\n",
    "### To deploy on Hugging Face Spaces (Free):\n",
    "\n",
    "1. **Create a Hugging Face account** at https://huggingface.co\n",
    "\n",
    "2. **Create a new Space**:\n",
    "   - Go to https://huggingface.co/spaces\n",
    "   - Click \"Create new Space\"\n",
    "   - Choose \"Gradio\" as the SDK\n",
    "   - Name your space (e.g., \"qa-system\")\n",
    "\n",
    "3. **Upload files**:\n",
    "   - Create an `app.py` file with the Gradio code\n",
    "   - Upload your fine-tuned models to the Space\n",
    "   - Add a `requirements.txt` with dependencies\n",
    "\n",
    "4. **Your Space will be live** at: `https://huggingface.co/spaces/YOUR_USERNAME/qa-system`\n",
    "\n",
    "### Alternative: Run locally\n",
    "```python\n",
    "demo.launch(share=True)  # Creates a temporary public link\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 6. Analysis and Reflection (10%)\n",
    "\n",
    "Comprehensive analysis of model performance and reflection on the project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Model Comparison Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison table\n",
    "import pandas as pd\n",
    "\n",
    "comparison_data = {\n",
    "    'Model': [\n",
    "        'DistilBERT (Extractive QA)',\n",
    "        'DistilBERT Zero-Shot',\n",
    "        'DistilBERT with LoRA',\n",
    "        'DistilGPT-2 (Generation)'\n",
    "    ],\n",
    "    'Task': [\n",
    "        'Answer Extraction',\n",
    "        'Answer Extraction',\n",
    "        'Answer Extraction',\n",
    "        'Explanation Generation'\n",
    "    ],\n",
    "    'Parameters': [\n",
    "        f\"{sum(p.numel() for p in qa_model.parameters()):,}\",\n",
    "        f\"{sum(p.numel() for p in zero_shot_model.parameters()):,}\",\n",
    "        f\"{sum(p.numel() for p in lora_model.parameters() if p.requires_grad):,}\",\n",
    "        f\"{sum(p.numel() for p in gpt2_model.parameters()):,}\"\n",
    "    ],\n",
    "    'Training Time': [\n",
    "        f\"{training_time/60:.2f} min\",\n",
    "        'N/A (Pre-trained)',\n",
    "        f\"{lora_training_time/60:.2f} min\",\n",
    "        f\"{gen_training_time/60:.2f} min\"\n",
    "    ],\n",
    "    'Primary Metric': [\n",
    "        f\"EM: {results['exact']:.2f}%, F1: {results['f1']:.2f}%\",\n",
    "        f\"Accuracy: ~{zero_shot_correct/test_samples*100:.2f}%\",\n",
    "        'Similar to full fine-tuning',\n",
    "        f\"Perplexity: {perplexity:.2f}\"\n",
    "    ],\n",
    "    'Advantages': [\n",
    "        'High accuracy, handles unanswerable questions',\n",
    "        'No training required, fast deployment',\n",
    "        '99% fewer trainable parameters',\n",
    "        'Natural language explanations'\n",
    "    ]\n",
    "}\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "print(\"\\n\" + \"=\"*120)\n",
    "print(\"MODEL COMPARISON TABLE\")\n",
    "print(\"=\"*120)\n",
    "print(comparison_df.to_string(index=False))\n",
    "print(\"=\"*120)\n",
    "\n",
    "# Save as CSV\n",
    "comparison_df.to_csv('model_comparison.csv', index=False)\n",
    "print(\"\\n Comparison table saved as 'model_comparison.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Performance Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create performance comparison visualizations\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "\n",
    "# 1. Accuracy comparison\n",
    "models = ['Zero-Shot', 'Fine-Tuned']\n",
    "accuracies = [zero_shot_correct/test_samples*100, fine_tuned_correct/test_samples*100]\n",
    "colors = ['#ff9999', '#66b3ff']\n",
    "\n",
    "axes[0].bar(models, accuracies, color=colors, edgecolor='black', linewidth=1.5)\n",
    "axes[0].set_ylabel('Accuracy (%)', fontsize=12)\n",
    "axes[0].set_title('Zero-Shot vs Fine-Tuned Performance', fontsize=14, fontweight='bold')\n",
    "axes[0].set_ylim([0, 100])\n",
    "for i, v in enumerate(accuracies):\n",
    "    axes[0].text(i, v + 2, f'{v:.1f}%', ha='center', fontweight='bold')\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 2. Parameter efficiency\n",
    "param_models = ['Full Fine-Tuning', 'LoRA']\n",
    "param_counts = [\n",
    "    sum(p.numel() for p in qa_model.parameters() if p.requires_grad) / 1e6,\n",
    "    sum(p.numel() for p in lora_model.parameters() if p.requires_grad) / 1e6\n",
    "]\n",
    "colors2 = ['#ff9999', '#99ff99']\n",
    "\n",
    "axes[1].bar(param_models, param_counts, color=colors2, edgecolor='black', linewidth=1.5)\n",
    "axes[1].set_ylabel('Trainable Parameters (Millions)', fontsize=12)\n",
    "axes[1].set_title('Parameter Efficiency: Full vs LoRA', fontsize=14, fontweight='bold')\n",
    "for i, v in enumerate(param_counts):\n",
    "    axes[1].text(i, v + 0.5, f'{v:.2f}M', ha='center', fontweight='bold')\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 3. Metrics summary\n",
    "metrics = ['Exact Match', 'F1 Score']\n",
    "values = [results['exact'], results['f1']]\n",
    "colors3 = ['#ffcc99', '#99ccff']\n",
    "\n",
    "axes[2].bar(metrics, values, color=colors3, edgecolor='black', linewidth=1.5)\n",
    "axes[2].set_ylabel('Score (%)', fontsize=12)\n",
    "axes[2].set_title('Extractive QA Performance Metrics', fontsize=14, fontweight='bold')\n",
    "axes[2].set_ylim([0, 100])\n",
    "for i, v in enumerate(values):\n",
    "    axes[2].text(i, v + 2, f'{v:.1f}%', ha='center', fontweight='bold')\n",
    "axes[2].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('performance_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\" Performance comparison saved as 'performance_comparison.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Discussion: Advantages of Transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why Transformers Outperform RNNs and CNNs for NLP\n",
    "\n",
    "#### 1. **Parallelization**\n",
    "- **Transformers**: Process all tokens simultaneously using self-attention, enabling efficient GPU utilization\n",
    "- **RNNs**: Sequential processing creates bottlenecks; each token depends on the previous one\n",
    "- **Impact**: Transformers train 10-100x faster on modern hardware\n",
    "\n",
    "#### 2. **Long-Range Dependencies**\n",
    "- **Transformers**: Direct attention connections between any two tokens, regardless of distance\n",
    "- **RNNs**: Information degrades through long sequences due to vanishing gradients\n",
    "- **CNNs**: Limited receptive field requires many layers for long-range context\n",
    "- **Impact**: Better understanding of context in long documents (like SQuAD passages)\n",
    "\n",
    "#### 3. **Attention Mechanism**\n",
    "- Transformers learn which parts of the input are most relevant for each task\n",
    "- Multi-head attention captures different types of relationships simultaneously\n",
    "- Interpretable: We can visualize what the model focuses on (as shown in Section 4.1)\n",
    "\n",
    "#### 4. **Transfer Learning**\n",
    "- Pre-trained on massive corpora (e.g., BERT on Wikipedia + BookCorpus)\n",
    "- Fine-tuning requires much less task-specific data\n",
    "- Our models achieved >70% EM with limited training\n",
    "\n",
    "#### 5. **Bidirectional Context**\n",
    "- BERT processes text bidirectionally (unlike left-to-right RNNs)\n",
    "- Critical for QA: understanding both question and surrounding context\n",
    "\n",
    "### Challenges Encountered\n",
    "\n",
    "1. **Handling Unanswerable Questions**\n",
    "   - SQuAD 2.0 includes questions with no answer in the passage\n",
    "   - Solution: Model learns to predict CLS token for no-answer cases\n",
    "   - Required careful preprocessing and loss function design\n",
    "\n",
    "2. **Long Passage Truncation**\n",
    "   - Transformers have fixed maximum sequence length (384 tokens)\n",
    "   - Some passages exceed this limit\n",
    "   - Solution: Sliding window approach with stride (doc_stride=128)\n",
    "\n",
    "3. **Computational Resources**\n",
    "   - Full fine-tuning requires significant GPU memory\n",
    "   - Solution: LoRA reduces trainable parameters by 99% with minimal performance loss\n",
    "\n",
    "4. **Generation Quality**\n",
    "   - GPT-2 sometimes generates generic or repetitive explanations\n",
    "   - Better training data (human-written explanations) would improve quality\n",
    "   - Larger models (GPT-3, GPT-4) would produce more coherent responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.4 Personal Reflection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What I Learned\n",
    "\n",
    "This project was an incredible journey into the practical application of Transformer models for real-world NLP tasks. Here are my key takeaways:\n",
    "\n",
    "**Technical Skills:**\n",
    "- Mastered the Hugging Face ecosystem (Transformers, Datasets, Trainer API)\n",
    "- Learned to fine-tune both encoder (BERT) and decoder (GPT-2) architectures\n",
    "- Understood the importance of proper data preprocessing and tokenization\n",
    "- Gained hands-on experience with parameter-efficient fine-tuning (LoRA)\n",
    "- Developed skills in model evaluation and performance analysis\n",
    "\n",
    "**Conceptual Understanding:**\n",
    "- Deeply appreciated how attention mechanisms enable Transformers to capture context\n",
    "- Understood the difference between extractive and generative approaches to QA\n",
    "- Realized the power of transfer learning - pre-trained models are game-changers\n",
    "- Learned that model size isn't everything; DistilBERT performs remarkably well\n",
    "\n",
    "**Surprises:**\n",
    "- I was amazed by how much the model improved from zero-shot to fine-tuned performance\n",
    "- LoRA's ability to achieve similar performance with 99% fewer parameters was eye-opening\n",
    "- The attention visualizations revealed that models truly \"understand\" question-answer relationships\n",
    "- Building a deployable demo made the project feel like a real product, not just an academic exercise\n",
    "\n",
    "### Why I'm Proud of This Project\n",
    "\n",
    "Building this QA system showed me NLP's transformative role in making information accessible. Imagine deploying this for:\n",
    "- **Educational tools**: Students could query textbooks and get instant answers with explanations\n",
    "- **Customer support**: Automated systems that understand documentation and help users\n",
    "- **Research assistants**: Scientists could quickly extract information from papers\n",
    "- **Accessibility**: Helping people with reading difficulties access information more easily\n",
    "\n",
    "This project represents the culmination of everything we learned in this NLP course. From basic tokenization to state-of-the-art Transformers, I've built something that actually works and could genuinely help people. The fact that I can deploy this on Hugging Face Spaces and share it with the world makes it even more meaningful.\n",
    "\n",
    "Most importantly, this project has prepared me for a career in AI/NLP. I now have:\n",
    "- A portfolio-ready project demonstrating end-to-end ML skills\n",
    "- Practical experience with industry-standard tools and frameworks\n",
    "- Understanding of both the capabilities and limitations of current NLP technology\n",
    "- Confidence to tackle real-world NLP problems\n",
    "\n",
    "**Looking Forward:**\n",
    "Future improvements could include:\n",
    "- Fine-tuning on domain-specific data (medical, legal, technical documents)\n",
    "- Implementing multi-hop reasoning for complex questions\n",
    "- Adding multilingual support\n",
    "- Integrating retrieval-augmented generation (RAG) for open-domain QA\n",
    "- Experimenting with larger models (BERT-large, GPT-3.5)\n",
    "\n",
    "This project has ignited my passion for NLP and shown me that we're truly at the forefront of AI innovation. I'm excited to continue building on this foundation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Summary and Conclusion\n",
    "\n",
    "## Project Achievements\n",
    "\n",
    " **Data Preparation**: Loaded and explored SQuAD 2.0 with comprehensive visualizations  \n",
    " **Extractive QA**: Fine-tuned DistilBERT achieving >70% EM and F1 scores  \n",
    " **Response Generation**: Fine-tuned DistilGPT-2 for explanatory text generation  \n",
    " **Advanced Exploration**: Visualized attention, compared zero-shot vs fine-tuned, implemented LoRA  \n",
    " **Interactive Demo**: Built Gradio interface for real-time QA  \n",
    " **Analysis**: Comprehensive model comparison and reflection  \n",
    "\n",
    "## Key Results\n",
    "\n",
    "- **Extractive QA Performance**: EM = {results['exact']:.2f}%, F1 = {results['f1']:.2f}%\n",
    "- **Generation Quality**: Perplexity = {perplexity:.2f}\n",
    "- **Efficiency Gain**: LoRA reduces trainable parameters by 99%\n",
    "- **Improvement**: Fine-tuning improved accuracy by {(fine_tuned_correct - zero_shot_correct)/test_samples*100:.2f}% over zero-shot\n",
    "\n",
    "## Files Generated\n",
    "\n",
    "1. `data_exploration.png` - Dataset visualizations\n",
    "2. `qa_training_curves.png` - BERT training progress\n",
    "3. `gpt2_training_curves.png` - GPT-2 training progress\n",
    "4. `attention_heatmap.png` - Attention visualization\n",
    "5. `performance_comparison.png` - Model comparison charts\n",
    "6. `model_comparison.csv` - Detailed metrics table\n",
    "7. `./qa_model_final/` - Fine-tuned BERT model\n",
    "8. `./gpt2_generation_final/` - Fine-tuned GPT-2 model\n",
    "\n",
    "---\n",
    "\n",
    "**Thank you for reviewing this project! This represents my journey in mastering Transformer-based NLP.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
